[["index.html", "Data Science Portfolio Chapter 1 Introduction", " Data Science Portfolio Chris J. Martin 2021-04-29 Chapter 1 Introduction intro text "],["predicting-the-amount-of-plastic-waste-a-country-produces-each-year.html", "Chapter 2 Predicting the amount of plastic waste a country produces each year 2.1 Data importing 2.2 Data cleaning 2.3 Exploratory data analysis 2.4 Modelling: A simple regression model 2.5 Applying the model to new data", " Chapter 2 Predicting the amount of plastic waste a country produces each year # core libraries library(tidyverse) library(rstanarm) # helper packages library(janitor) # for clean names library(bayesplot) # posterior predictive graphic checks # global settings theme_set(theme_light()) In this notebook I try out and apply some of the concepts, tools and techniques I have been learning as I read through the excellent Regression and Other Stories. To do this I needed a dataset to work with. I decided to work with some data I had found (via #TidyTuesday) on how much plastic waste countries produce. So with this dataset to hand, my more specific aims were to: To develop a simple (Bayesian) regression model to predict the amount of plastic waste created by a country; To use appropriate metrics to compare the performance of several regression models; To use a simple regression to predict future plastic waste production. In his introductory section (1), I add a few reflections on what I learnt throughout the process of working with the plastic waste data and developing a simple regression model. The following sections of the notebook then address in turn (2) data import, (3) data cleaning, (4) exploratory data analysis, (5) model fitting and evaluation, and (6) applying the model to make predictions. It quickly became clear to me that I was probably not actually working with observed data of how much plastic waste countries had produced in 2011. Rather I was actually working with the outputs of model which estimated how much plastic waste countries had produced in 2011. So, in a very simplistic way I was actually trying to recreate a model developed by a team of researchers (Jambeck et al. 2015) probably over the course of months and months of work. Fortunately, I had not read the paper detailing the construction of the model I was in effect trying to recreate. If I had read it I would have known in advance too much about the predictors to use, which in turn might well have reduced my opportunities for learning. The simple two predictor model developed in this notebook used a countries population and GDP to estimate its annual plastic waste production. Disappointingly, the model developed performed badly when attempting to make predictions for countries which produce large amounts of plastic waste. However, the process of developing the model provided a great opportunity for me to develop more hand-on experience of the practicalities of regression modeling. The next steps to developing a more nuanced and hopefully more effective model for predicting a countrys annual plastic waste production would be to review approaches used to developing plastic waste models by Jambeck et al. 2015 and other researchers. This would hopefully provide me with greater insights into which variables (beyond population and GDP) would be helpful in estimating a countrys annual plastic waste production. The dataset used: All data files are located in /data/ File Name Variable name in this notebook Notes ../data/waste_vs_gdp.csv waste_vs_gdp* Data provided by #tidytuesday via our world in data; original sources: Jambeck et al. 2015 and gapminder. 2.1 Data importing The data was provided by #TidyTuesday, a collaborative weekly data science project run the R for Data Science Community. Each week a dataset is posted for data scientists to practice their data wrangling and visualisation skills. In May 2019 #TidyTuesday posted three datasets (sourced from our world in data) providing three datasets focussing on plastic waste production, coastal populations and economic activity. The data is provided (mainly) at the country scale. In this analysis I work with only one of the three datasets. Below I read in the dataset relating to plastic waste production and GDP and quickly clean up some of the variable names to make them easier to work with. # read dataset waste_vs_gdp &lt;- read_csv(&#39;data/waste_vs_gdp.csv&#39;) %&gt;% clean_names() %&gt;% rename(country = entity, country_code = code, population = total_population_gapminder, gdp_per_cap = gdp_per_capita_ppp_constant_2011_international_constant_2011_international, plastic_waste_per_cap = per_capita_plastic_waste_kilograms_per_person_per_day) 2.2 Data cleaning 2.2.1 Missing data I started the analysis with the idea that there could be relationship between the amount of plastic waste produced by a country and the size of its economy. So, I focused on just one of the three #TidyTuesday datasets waste_vs_gdp. # show data for initial inspection waste_vs_gdp ## # A tibble: 22,204 x 6 ## country country_code year plastic_waste_per_cap gdp_per_cap population ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan AFG 1800 NA NA 3280000 ## 2 Afghanistan AFG 1820 NA NA 3280000 ## 3 Afghanistan AFG 1870 NA NA 4207000 ## 4 Afghanistan AFG 1913 NA NA 5730000 ## 5 Afghanistan AFG 1950 NA NA 8151455 ## 6 Afghanistan AFG 1951 NA NA 8276820 ## 7 Afghanistan AFG 1952 NA NA 8407148 ## 8 Afghanistan AFG 1953 NA NA 8542906 ## 9 Afghanistan AFG 1954 NA NA 8684494 ## 10 Afghanistan AFG 1955 NA NA 8832253 ## # ... with 22,194 more rows # visualise missing data across variables waste_vs_gdp %&gt;% arrange(country, year) %&gt;% visdat::vis_dat() Quickly looking at the data and plot above: I am happy the data types for each variable are appropriate and that the variables look to be in tidy format. I can see that it is presented as a longitudinal dataset with each row consisting of an observation for a country (or another entity - e.g. a group of countries) for a specific year. However, the variable I am interesting in predicting (plastic waste) most of the data is missing  There is data on plastic waste production for 2010 (see below). So, data for other years can be removed. # find out what plastic waste data is actually available waste_vs_gdp %&gt;% filter(!is.na(plastic_waste_per_cap)) %&gt;% distinct(year) ## # A tibble: 1 x 1 ## year ## &lt;dbl&gt; ## 1 2010 # remove observations where there is no plastic waste data available waste_vs_gdp_cleaned &lt;- waste_vs_gdp %&gt;% filter(!is.na(plastic_waste_per_cap)) Taking another at the missing data (just for the year 2010): I can see there is some missing data for gdp_per_cap . Im thinking that this variable will be used as a predictor later on in a linear regression model, so I looked at the missing data in a more detail. Based on a quick inspect, the 38 entities without gdp_per_cap data (see below) look to fall into two categories: small dependencies, territories and islands which form part of larger countries, for example Gibraltar which is part of the UK. countries where is it might be difficult to quantify GDP, for example North Korea and Cuba. Although, it means losing approximately 20% of the dataset I think the best option is to omit these countries from the analysis. As at the moment I cant think of a way to reliably impute GDP values for these 38 entities (particularly as they may have different economic characteristics from typical larger countries). # look again at where things are at in terms of missing data waste_vs_gdp_cleaned %&gt;% visdat::vis_miss() # check which countries/entities have no GDP data waste_vs_gdp_cleaned %&gt;% filter(is.na(gdp_per_cap)) %&gt;% distinct(country, country_code) ## # A tibble: 38 x 2 ## country country_code ## &lt;chr&gt; &lt;chr&gt; ## 1 Anguilla AIA ## 2 Aruba ABW ## 3 British Virgin Islands VGB ## 4 Cayman Islands CYM ## 5 Channel Islands OWID_CIS ## 6 Christmas Island CXR ## 7 Cocos Islands CCK ## 8 Cook Islands COK ## 9 Cuba CUB ## 10 Curacao CUW ## # ... with 28 more rows # remove countries with no GDP data waste_vs_gdp_cleaned &lt;- waste_vs_gdp_cleaned %&gt;% filter(!is.na(gdp_per_cap)) So, looking for the last time at missing data there is just the missing data for population to consider: As it is just three countries without population data I am comfortable with omitting these from the analysis. # look again at where things are at in terms of missing data waste_vs_gdp_cleaned %&gt;% visdat::vis_miss() # check which entities/countries don&#39;t have population data waste_vs_gdp_cleaned %&gt;% filter(is.na(population)) %&gt;% distinct(country, country_code) ## # A tibble: 3 x 2 ## country country_code ## &lt;chr&gt; &lt;chr&gt; ## 1 Micronesia (country) FSM ## 2 Palestine PSE ## 3 Yemen YEM # remove observations without population information waste_vs_gdp_cleaned &lt;- waste_vs_gdp_cleaned %&gt;% filter(!is.na(population)) 2.2.2 Inspecting the data for the country variable Given the raw data used the variable name (geographic) entity rather than country I wanted to check that the cleaned data only contains countries. Given there are only 145 observations remaining in the dataset, I did this check manually. There were no obvious non-country entities (e.g. continents such as Europe or Asia, or high/middle/low income country groups) remaining. # confirm there are no non-country entities within the dataset waste_vs_gdp_cleaned %&gt;% select(country, country_code) ## # A tibble: 145 x 2 ## country country_code ## &lt;chr&gt; &lt;chr&gt; ## 1 Albania ALB ## 2 Algeria DZA ## 3 Angola AGO ## 4 Antigua and Barbuda ATG ## 5 Argentina ARG ## 6 Australia AUS ## 7 Bahamas BHS ## 8 Bahrain BHR ## 9 Bangladesh BGD ## 10 Barbados BRB ## # ... with 135 more rows I also confirmed there are no duplicates of countries within the dataset. # confirm there are no duplicate countries assertthat::are_equal(nrow(waste_vs_gdp_cleaned), nrow(waste_vs_gdp_cleaned %&gt;% distinct(country)) ) ## [1] TRUE 2.3 Exploratory data analysis The plastic waste and gdp data are represented on a per capita basis (plastic_waste_per_cap and gdp_per_cap respectively). During the modelling process I think it will be useful to have the absolute values for plastic waste and gdp. Fortunately, population data is provide so it is easy to calculate these figures. Having calculated plastic_waste_tot, I think it make sense to adjust the units to tonnes of waste per year (plastic_waste_per_cap was in kg per day per person). Just to confirm that the transformation I sum plastic_waste_tot to give global plastic waste for 2010 (which is approx. 269 million tonnes ). This is close enough to the headline figure of 275 million tonnes of global plastic waste for 2010 given by Jambeck et al. (), to be confident nothing major has gone wrong in my calculation. # create new variables for eda and regression waste_vs_gdp_eda &lt;- waste_vs_gdp_cleaned %&gt;% mutate(plastic_waste_tot = plastic_waste_per_cap * population, gdp = gdp_per_cap * population, # adjust units plastic_waste_tot = (plastic_waste_tot / 1000) * 365 ) # calculate global plastic waste total waste_vs_gdp_eda %&gt;% summarise(sum(plastic_waste_tot)) %&gt;% pull() ## [1] 268899757 From the summary statistics (see below) for the outcome variable (plastic_waste_tot) and the two potential predictor variables (gdp and population), it looks like all three will be heavily right-skewed. And, this is confirmed by the plots below which apply a log scale to the x axis. So, I start with a little bit of very basic feature engineering and take logs of the three variables to produce outcome and predictors with more normal-like distributions. # produce summary statistics waste_vs_gdp_eda %&gt;% select(-year) %&gt;% skimr::skim() %&gt;% skimr::yank(&quot;numeric&quot;) Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist plastic_waste_per_cap 0 1 2.000000e-01 3.100000e-01 0.01 9.000000e-02 1.400000e-01 2.200000e-01 3.600000e+00  gdp_per_cap 0 1 1.924042e+04 2.018923e+04 660.21 4.984190e+03 1.212384e+04 2.922199e+04 1.251408e+05  population 0 1 4.338416e+07 1.546056e+08 9827.00 1.341140e+06 6.192993e+06 3.167159e+07 1.341335e+09  plastic_waste_tot 0 1 1.854481e+06 6.075795e+06 516.51 6.079521e+04 3.322850e+05 1.355185e+06 5.924007e+07  gdp 0 1 5.966166e+11 1.805967e+12 29347842.73 1.247826e+10 6.955141e+10 3.872508e+11 1.532495e+13  # define labels for plotting x_lab_pop &lt;- &quot;Plastic waste produced (tonnes per year)&quot; x_lab_gdp &lt;- &quot;\\n2010 GDP (USD)\\n\\nadjusted for inflation (2011 benchmark year) and purchaing power parity&quot; y_lab &lt;- &quot;Number of countries&quot; caption_lab &lt;- &quot;Source: ourworldindata / gapminder&quot; # plot histograms for the two predictors and the outcome p &lt;- ggplot(waste_vs_gdp_eda) p + geom_histogram(aes(plastic_waste_tot), colour = &quot;black&quot;) + scale_x_log10(labels = scales::comma) + labs(x = x_lab_pop, y = y_lab, caption = caption_lab ) p + geom_histogram(aes(gdp), colour = &quot;black&quot;) + scale_x_log10(labels = scales::label_number_si(accuracy=0.1)) + labs(x = x_lab_gdp, y = y_lab, caption = caption_lab) p + geom_histogram(aes(population), colour = &quot;black&quot;) + scale_x_log10(labels = scales::comma) + labs(x = &quot;Population&quot;, y = y_lab, caption = caption_lab) # log transform skewed variables waste_vs_gdp_eda &lt;- waste_vs_gdp_eda %&gt;% mutate(across(.cols = population:gdp, .fns = log, .names = &quot;log_{.col}&quot;) ) There is a high degree of correlation between the logs each of the predictor variables (population and gdp) and the log of the outcome variable (plastic_waste_tot)(see below). This is promising in terms of developing a predictive linear regression model). However, there is also evidence of multicollinearity (i.e. a high degree of correlation between the two predictor variable). If both are included in a linear regression model there is the potential for standard errors of the regression coefficients to increase (relative to a model containing only one of the two predictors). This is something to come back to in the comparision of model performance below. # pair plot to review correlation between predictors and outcome variables GGally::ggpairs(waste_vs_gdp_eda %&gt;% select(starts_with(&quot;log_&quot;))) 2.4 Modelling: A simple regression model 2.4.1 Model fitting Below I fit three linear models (using a Bayesian linear regression algorithm from the rstanarm package) with plastic_waste_tot as the outcome variable. The three models have different predictors as follows: Using gdp as a single predictor; Using population as a single predictor; Using both gdp and population as predictors. Reviewing the outputs of the three model fits I noted the following: The residual standard deviation (sigma in the output below) is highest for model 1 and lowest for model 3. The uncertainty around the values of the regression coefficients (MAD_SD in the output below) is smallest for model 2. Below I explore these two points further by visualising the residuals and the uncertainty around the regression coefficients. # reassign data at the start of modelling section waste_vs_gdp_mod &lt;- waste_vs_gdp_eda # fit models lm_1 &lt;- stan_glm(formula = log_plastic_waste_tot ~ log_gdp, data = waste_vs_gdp_mod, refresh = 0) lm_2 &lt;- update(lm_1, formula = log_plastic_waste_tot ~ log_population) lm_3 &lt;- update(lm_1, formula = log_plastic_waste_tot ~ log_gdp + log_population) # display model outputs lm_1 ## stan_glm ## family: gaussian [identity] ## formula: log_plastic_waste_tot ~ log_gdp ## observations: 145 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) -7.1 0.7 ## log_gdp 0.8 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.9 0.1 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg lm_2 ## stan_glm ## family: gaussian [identity] ## formula: log_plastic_waste_tot ~ log_population ## observations: 145 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) -1.3 0.5 ## log_population 0.9 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.8 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg lm_3 ## stan_glm ## family: gaussian [identity] ## formula: log_plastic_waste_tot ~ log_gdp + log_population ## observations: 145 ## predictors: 3 ## ------ ## Median MAD_SD ## (Intercept) -4.8 0.6 ## log_gdp 0.4 0.0 ## log_population 0.5 0.1 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.7 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg 2.4.2 Uncertainty in the model fit For the first two models (those with just a single continuous predictor) it is straight-forward to plot the regression line alongside the raw data. The uncertainty associated with the regression line was added by plotting randomly sampled draws from the posterior distribution (for the regression coefficients). For both these simple single predictor models a good fit between the model and the data can be observed. Additionally the uncertainty around the intercept and gradient terms of the regression line appears to be relatively limited. # define a function for plotting a regression line with # draws from posterior distribution plot_lm_with_uncertainty &lt;- function(model, data, x, y){ lm_coef &lt;- coef(model) # sample draws from the posterior distribution posterior_coef_draws &lt;- as_tibble(model) %&gt;% rename(a = `(Intercept)`, b = {{x}}) %&gt;% sample_n(100) # produce regression plot (including uncertainty) ggplot(data, aes({{x}}, {{y}})) + geom_point() + geom_abline(data = posterior_coef_draws, mapping = aes(intercept = a, slope = b), colour = &#39;skyblue&#39;, alpha = 0.3, size = 0.2) + # a sample of posterior to show uncertainty geom_abline(intercept = lm_coef[1], slope = lm_coef[2], size = 1) # estimated regression line } # Produce plots of fitted model (+ uncertainty) plot_lm_with_uncertainty(lm_1, waste_vs_gdp_mod, log_gdp, log_plastic_waste_tot) + labs(x = &quot;Log GDP&quot;, y = &quot;Log total annual plastic waste&quot;, subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp)&quot;) plot_lm_with_uncertainty(lm_2, waste_vs_gdp_mod, log_population, log_plastic_waste_tot) + labs(x = &quot;Log Population&quot;, y = &quot;Log total annual plastic waste&quot;, subtitle = &quot;lm_2 (log_plastic_waste_tot ~ log_population)&quot;) Additionally, I quickly looked at the uncertainty around the parameters of each of three the models (i.e. uncertainty around the intercept, and the coefficients for the predictor/s). See plots below, where the point represents the central estimate for the parameter, and the thick dark blue line represents a 50% uncertainty interval and the thin blue line a 90% uncertainty interval. Reviewing the plots for the three models I noted that: In all the models there is very little uncertainty associated with the co-efficents of the predictor variable/s. So, the uncertainty around the parameterisation of each model is effectively restricted to uncertainty around the value of the intercept of the regression line. # define a function for producing plots of model parameter uncertainty plot_coef_uncertainty_int &lt;- function(mod, ...){ # process posterior for interval plots posterior &lt;- as.array(mod) # plot uncertainty around model parameters mcmc_intervals(posterior, ...) } # produce plots of model parameter uncertainty plot_coef_uncertainty_int(lm_1, pars = c(&quot;(Intercept)&quot;, &quot;log_gdp&quot;)) + labs(subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp)&quot;) plot_coef_uncertainty_int(lm_2, pars = c(&quot;(Intercept)&quot;, &quot;log_population&quot;)) + labs(subtitle = &quot;lm_2 (log_plastic_waste_tot ~ log_population)&quot;) plot_coef_uncertainty_int(lm_3, pars = c(&quot;(Intercept)&quot;, &quot;log_gdp&quot;, &quot;log_population&quot;)) + labs(subtitle = &quot;lm_3 (log_plastic_waste_tot ~ log_gdp + log_population)&quot;) 2.4.3 Reviewing the residuals Looking at the residuals for each of the three models (see below) a similar pattern is present. As the amount of plastic waste produced by a country grows so do the residuals. In all three models the residuals diverge from 0 from predictions of approximately 0.5 million tonnes of plastic per year upwards. This suggest that for countries producing larger amounts of plastic the models are # set theme again as seemed to be getting forgetten for some reason theme_set(theme_light()) # calculate residuals residuals &lt;- waste_vs_gdp_mod %&gt;% mutate(pred_lm_1 = exp(predict(lm_1)), pred_lm_2 = exp(predict(lm_2)), pred_lm_3 = exp(predict(lm_3))) %&gt;% select(plastic_waste_tot, starts_with(&quot;pred_&quot;)) %&gt;% mutate(resid_lm_1 = plastic_waste_tot - pred_lm_1, resid_lm_2 = plastic_waste_tot - pred_lm_2, resid_lm_3 = plastic_waste_tot - pred_lm_3) # define residual plotting functions plot_residuals &lt;- function(data, x, y, ...){ ggplot(residuals, aes({{x}}, {{y}})) + geom_hline(yintercept = 0, colour = &quot;grey50&quot;) + geom_point() + scale_x_log10(labels = scales::comma) + #coord_cartesian(ylim = c(-4.25e7, 4.25e7)) + labs(y = &quot;Residual\\n(Tonnes of plastic waster per year)&quot;) } plot_residuals(residuals, pred_lm_1, resid_lm_1) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp)&quot;) plot_residuals(residuals, pred_lm_2, resid_lm_2) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_2 (log_plastic_waste_tot ~ log_population)&quot;) plot_residuals(residuals, pred_lm_3, resid_lm_3) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_3 (log_plastic_waste_tot ~ log_gdp + log_population)&quot;) I was wondering if the trend above would remain if I plotted the residuals as a proportion of the observed value (see below). This allowed me to get a sense of the errors in the models accounting for the wide variation in the amounts of plastic waste produced by the countries in the dataset (ranging from below 1000 tonnes per year to over 10 million tonnes). There is some indication in the plots below that all three of the models perform better for countries producing small amounts of plastic waste and worse for countries producing large amounts. However, the trend is less pronounced than in the plots of the raw residuals. #define a function for plotting residuals as a proportion of observed values plot_residuals_perc &lt;- function(data, x, y, ...){ ggplot(residuals, aes({{x}}, {{y}} / plastic_waste_tot)) + geom_hline(yintercept = 0, colour = &quot;grey50&quot;) + geom_point() + scale_x_log10(labels = scales::comma) + labs(y = &quot;Residual / observed value&quot;) } # plot residuals as a proportion of observed values plot_residuals_perc(residuals, pred_lm_1, resid_lm_1) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp)&quot;) plot_residuals_perc(residuals, pred_lm_2, resid_lm_2) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_population)&quot;) plot_residuals_perc(residuals, pred_lm_3, resid_lm_3) + labs(x = &quot;predicted value&quot;, subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp + log_population)&quot;) 2.4.4 Bayesian R squared Next I took a look at the R2 values for each of three models. Just to note as this is a Bayesian regression analysis, the calculation of R2 is slight different from the standard (frequentist) calculation. Bayesian R2 is defined as: \\(\\frac{\\text{Explained Variance}}{\\text{Explained Variance + Residual Variance}} = \\frac{var_{fit}}{var_{fit} + var_{res}}\\) The table below shows the model three (including both gdp and population as predictors) has the largest R2 value and hence explain more of the variance in the dataset than the other two models. In histograms showing draws from the posterior distribution of Bayesian R2 there are some overlaps between the lower tail of model 3, and the upper tails of models 1 and 2. Ill leave formal analysis of confidence intervals for the R2 values, as these overlaps appear to be relatively small. # display Bayesian R2 values for the three models tribble( ~model, ~r2, 1, median(bayes_R2(lm_1)), 2, median(bayes_R2(lm_2)), 3, median(bayes_R2(lm_3)) ) %&gt;% kableExtra::kbl() %&gt;% kableExtra::kable_styling() model r2 1 0.8499685 2 0.8701244 3 0.9089521 # define r2 plots plot_bayes_r2 &lt;- function(mod){ bayes_R2(mod) %&gt;% as_tibble() %&gt;% ggplot(aes(value)) + geom_histogram(colour = &quot;black&quot;) + geom_vline(aes(xintercept = median(value)), colour = &quot;black&quot;, size = 2) + labs(x = &quot;Bayesian R Squared&quot;, y = &quot;Number of draws from posterior&quot;) + theme_light() } # produce plots for the three models plot_bayes_r2(lm_1) + labs(subtitle = &quot;lm_1 (log_plastic_waste_tot ~ log_gdp)&quot;) plot_bayes_r2(lm_2) + labs(subtitle = &quot;lm_2 (log_plastic_waste_tot ~ log_population)&quot;) plot_bayes_r2(lm_3) + labs(subtitle = &quot;lm_3 (log_plastic_waste_tot ~ log_gdp + log_population&quot;) 2.4.5 Cross validation and log scores To assess the likely relative performance of the three models on out-of-sample data I ran LOO (leave-one-out) cross-validation for each model. The accuracy of the each models predictions across the cross validations was compared using log scores. The output of the log score comparison (see below) shows that model 3 (including gdp and population as predictors) offered the best performance. With model 1 (only gdp as a predictor) offering the worst performance of the three models. The relative performance of the models in cross validation is the same could be see above when considering the residual standard deviation and R2 values for each model. So, we can be relatively confident that model 3 is the best performing of the three simple models. # run loo cross validation loo_1 &lt;- loo(lm_1) loo_2 &lt;- loo(lm_2) loo_3 &lt;- loo(lm_3) # compare performance of model using log scores loo_compare(loo_1, loo_2, loo_3) ## elpd_diff se_diff ## lm_3 0.0 0.0 ## lm_2 -24.6 8.3 ## lm_1 -35.2 9.0 2.4.6 Graphical posterior predictive checks Focusing now just on the model that performed best in the leave-one-out cross-validation tests above (model 3 - which includes the two predictors gdp and population). I completed some basic graphical posterior predictive checks. The idea behind these checks is that a model that is performing reasonable well should be able to produce data with a distribution that reasonably closely resembles the distribution of the observed (i.e. in-sample data). The plot below shows the density estimate for observed values of (log) total annual plastic waste in dark blue (labeled as y). Also shown (in light blue) are density estimates for predicted values of (log) total annual plastic waste. 75 randomly selected draws from the posterior distribution of regression coefficients (labeled as yrep) are shown to give a sense of the uncertainty in the model predictions. This visual comparison shows that the density of observed values has a broader, wider peak than the model values. So, in effect the model is pushing prediction toward the center of the distribution a bit too much. If I was performing a more in-depth study it would be worth exploring further if the model could be developed further to produce predictions with a distribution more closely resembling the observed values. # prepare vectors for using bayesplot functions y &lt;- waste_vs_gdp_mod$log_plastic_waste_tot # observed values of y y_rep &lt;- posterior_predict(lm_3, draws = 500) # predicted values of v # for 500 draws from the posterior # confirm dimensions are as expected dim(y_rep) ## [1] 500 145 # plot color_scheme_set(&quot;brightblue&quot;) ppc_dens_overlay(y, # sample 50 rows of matrix at random y_rep[sample(nrow(y_rep), size = 75),]) + labs(x = &quot;Log total annual plastic waste&quot;, y = &quot;Density estimate&quot;, subtitle = &quot;Observed data: y (dark blue)\\nModel prediction: y_rep (light blue)&quot;) To understand in a little more details how the model is performing I looked at three test statistics (the median, maximum and minimum values). For each statistic the observed value (shown as a single vertical line, labeled T(y)) is overlaid on a histogram showing the predicted values produced by a sample of draws from the posterior predictive. Reviewing the three plots for the three test statistics I noted: The observed test statistics values are approximately in the center of the distributions of test statistics produced by the model. So, the model is predicting median, min and maximum values fairly accurately. However, the observed maximum is slightly displaced from the center of the distribution produced by the model. Suggesting that model has a tendency to under-predict for countries where the largest amount of plastic waste are produced. # define a function for adding standard labels to test statistic plots test_stat_labs &lt;- function(){ list(labs(x = &quot;Log(total annual plastic waste)&quot;, y = &quot;Number of draws from posterior\\npredictive distriubtion&quot;, subtitle = &quot;Observed data: y (dark blue)\\nModel prediction: y_rep (light blue)&quot;)) } # define functions for test statistics max_value &lt;- function(x) max(x) min_value &lt;- function(x) min(x) median_value &lt;- function(x) median(x) # plot districutions of test statistics ppc_stat(y, y_rep, stat = median_value) + test_stat_labs() ppc_stat(y, y_rep, stat = max_value) + test_stat_labs() ppc_stat(y, y_rep, stat = min_value) + test_stat_labs() 2.4.7 Summary of the model fitting process I think the key points to draw out of analysis of the model fit presented above are as follows. The two predictor model (including gdp and population as predictors ) produced more accurate in-sample predictions than either of the models containing only one predictor. The two predictors are closely correlated , however than did not appear to de-stablise the model. Leave-one-out cross validation showed that the two predictor model is also likely to perform better on out-of-sample data than either of the other one predictor models. There is little uncertainty around the parameters (intercept and the predictor coefficients) of the two predictor model. The residuals for the two predictor model show the accuracy of the model declining as the value of y (i.e. total annual amount of plastic waste produced by a country) increases. Graphical posterior predictive checks for the two predictor model showed a tendency for the model make predictions with higher narrower peak than the observed data. 2.5 Applying the model to new data The regression model for predicting the annual amount of plastic waste produced by country (based on its GDP and population) developed above considered a single year of observed data (2010). This was because it was the only year for which data was available for the outcome variable (annual amount of plastic waste) and the predictor variables (GDP and population). However, within the dataset there is data available for the predictors for the period 2011 to 2013 (inclusive). So, I thought it would be interesting to apply the model over this period to (retrospectively) predict the total global production. Before doing so, I need to return to the original data and process it to be used in this prediction exercise. Due to gaps in the gdp and population data, I decided just to focus on just the four highest plastic waste producing countries in 2010. # identify the top n waste producers in 2010 top_waste_2010 &lt;- waste_vs_gdp_mod %&gt;% slice_max(order_by = plastic_waste_tot, n = 4) top_waste_2010_countries &lt;- top_waste_2010 %&gt;% select(country) %&gt;% pull() # process source data to focus on 2011 to 2013 where # data for both predictors is available pred_plastic_waste &lt;- waste_vs_gdp %&gt;% filter(year &gt; 2010, country %in% top_waste_2010_countries) %&gt;% mutate(gdp = gdp_per_cap * population, log_gdp = log(gdp), log_population = log(population)) %&gt;% filter(!is.na(gdp)) %&gt;% select(-plastic_waste_per_cap) Then I used the model to predict annual plastic waste production for these four countries for 2011-2013. # calculate the median prediction from the posterior predictive distribution preds &lt;- posterior_predict(lm_3, newdata = pred_plastic_waste) A little more data wrangling is need to get everything into shape for plotting. # process predictions for plotting plotting_df &lt;- pred_plastic_waste %&gt;% bind_cols(as_tibble(t(preds))) %&gt;% pivot_longer(cols = V1:last_col(), names_to = &quot;draw_num&quot;, values_to = &quot;pred&quot;) Finally, I can plot the predictions of the model for 2011-2013 for the four of the top waste producing countries. These are shown as combined violin and boxplots showing the distributions of predicted annual plastic waste production draw from posterior predictive. The central line of the boxplots shows the median which can be taken as a point estimate of the models prediction. The blue points show the observed value of annual plastic waste production calculated from the original dataset. In terms of the validity of the model predictions two points jump out when reviewing the plot. Considering the median prediction the model shows the expected trend over 2011-2013 for each country, with a small steady rise in plastic production. However, comparing the observed data (i.e. the blue point) with the prediction shows the model is systematically underestimating the amounts of plastic waste produced in each country. In retrospect, this issue could have been anticipated as the analysis of the model residuals (see above) showed a strong tendency for the model to underestimate growing as the scale of plastic waste production grows. # produce plot plotting_df %&gt;% ggplot(mapping = aes(x = year, y = exp(pred), group = year)) + geom_violin(fill = &quot;grey70&quot;, colour = NA) + geom_boxplot(alpha = 0, width = 0.1) + geom_point(data = top_waste_2010, mapping = aes(x = year, y = plastic_waste_tot), size = 2, colour = &quot;blue&quot;) + scale_y_log10(labels = scales::label_number_si(accuracy=0.1)) + facet_wrap(~ factor(country, levels = c(&quot;China&quot;, &quot;United States&quot;, &quot;Germany&quot;, &quot;Brazil&quot;)), scales = &quot;free_y&quot;) + labs(x = NULL, y = &quot;Predicted annual plastic waste production (tonnes)\\n&quot;) "],["estimating-the-proportion-of-plastic-waste-countries-mismanage.html", "Chapter 3 Estimating the proportion of plastic waste countries mismanage 3.1 Data importing and cleaning 3.2 Exploratory data analysis 3.3 Modelling: Piecewise linear regression 3.4 Reflections and next steps", " Chapter 3 Estimating the proportion of plastic waste countries mismanage In this notebook I am again working with some datasets I had found (via #TidyTuesday) which detail how much plastic waste countries produce. See the table below for a few more details on the datasets including the source of the data. My objective was to develop a model estimates the proportion of plastic waste mismanaged by a country. There is limited documentation available with the datasets, so at the moment I am assuming that mismanagement is being used as a synonym for various forms of disposal activities that have a high environmental impact (dumping plastics in the sea etc.). I started the modeling process with the idea that GDP per capita might be a helpful feature for estimating the proportion of plastic waste mismanaged. This idea was based economic theories (such as Jevons paradox) that suggest: on one hand that as economies develop their environmental impact can increase through increased resource consumption. While on the other hand, that as economies develop they can use resources more efficient, so reducing the environmental impact of each unit of a resource that is consumed. This notebook has five sections: (This Introduction section) outlining the basic ideas that motivated the analysis; Importing and cleaning the two datasets needed to develop a model. Conducting an exploratory data analysis to understand the association between the proportion of plastic waste mismanaged by a country and its GDP per capita. Developing a piecewise linear regression model to estimate he proportion of plastic waste mismanaged by a country based on its GDP per capita. Reflecting on the modeling process and identifying next steps. Dataset used: File Names Variable name in this notebook Notes data/waste_vs_gdp.csv data/mismanaged_vs_gdp.csv waste_vs_gdp mismanaged_vs_gdp Data provided by #tidytuesday via our world in data; original sources: Jambeck et al. 2015 and gapminder. 3.1 Data importing and cleaning In order to understand the proportion of plastic waste mismanaged by country, I needed to import two datasets: (1) focuses on the total amount of plastic waste produced; and, (2) focuses on the amount of plastic waste mismanaged. Below I do some very basic cleaning of the data around the variable names to make them easier to work with in the analysis and modeling. # read in the dataset on waste and gdp waste_vs_gdp &lt;- read_csv(&#39;data/waste_vs_gdp.csv&#39;) %&gt;% # make variables names consistent in terms of case etc. clean_names() %&gt;% # clean up variable names including shortening variables names to make them # easier to work with rename(country = entity, country_code = code, population = total_population_gapminder, gdp_per_cap = gdp_per_capita_ppp_constant_2011_international_constant_2011_international, plastic_waste_per_cap = per_capita_plastic_waste_kilograms_per_person_per_day) # read in the dataset on mismanaged waste and gdp mismanaged_vs_gdp &lt;- read_csv(&#39;data/mismanaged_vs_gdp.csv&#39;) %&gt;% # make variables names consistent in terms of case etc. clean_names() %&gt;% # clean up variable names including shortening variables names to make them # easier to work with rename(country = entity, country_code = code, population = total_population_gapminder, mismanaged_plastic_per_cap = per_capita_mismanaged_plastic_waste_kilograms_per_person_per_day, gdp_per_cap = gdp_per_capita_ppp_constant_2011_international_rate) I know from having previously worked with these datasets that plastic waste data is only available for 2010. Also, there are a few countries with missing values for plastic waste, GDP and population variables. So, in the code below I process the two datasets according removing observations for years other than 2010, and observations where missing data could provide problematic during the modeling process. Then all that remains is to joining the two datasets, which is easy enough given they come from the same source and share primary keys. # focus down on countries where plastic waste and gdp data is available waste_vs_gdp_2010 &lt;- waste_vs_gdp %&gt;% filter(year == 2010, !is.na(plastic_waste_per_cap), !is.na(gdp_per_cap)) # focus down on countries where mismanaged waste and gdp data is available mismanaged_vs_gdp_2010 &lt;- mismanaged_vs_gdp %&gt;% filter(year == 2010, !is.na(mismanaged_plastic_per_cap), !is.na(gdp_per_cap)) # join the two datasets ahead of analysis waste_mismanaged_gdp &lt;- waste_vs_gdp_2010 %&gt;% left_join(mismanaged_vs_gdp_2010) 3.2 Exploratory data analysis I started this analysis with a fairly clear idea of what I wanted to do, based on: some theoretical ideas around economic development and environmental impact; and, a previous exploratory data analysis I done with the plastic waste data. So, in this notebook I will keep the exploratory data analysis fairly focused on the task at hand: estimating the proportion of plastic waste mismanaged by country based on economic data available. The first I thing I need to do is create a few new variables (see code below). This allows me to then calculated the proportion of plastic waste mismanaged by a country (i.e. the variable to estimated). # reshape data to create target feature waste_mismanaged_gdp_plot &lt;- waste_mismanaged_gdp %&gt;% mutate(plastic_waste_tot = plastic_waste_per_cap * population, misman_plastic_waste_tot = mismanaged_plastic_per_cap * population, prop_plastic_waste_misman = misman_plastic_waste_tot / plastic_waste_tot) Finally before creating some exploratory visualisation, I just needed to identify the top waste producing countries so these can be labeled within the plot. # identify top n waste producing country for labelling in plot top_waste_producing_countries &lt;- waste_mismanaged_gdp_plot %&gt;% slice_max(plastic_waste_tot, n = 10) # set colour palette for the plot plot_colour_pal &lt;- RColorBrewer::brewer.pal(9, &quot;Blues&quot;) text_colour &lt;- plot_colour_pal[9] bg_colour &lt;- plot_colour_pal[2] # specify plot annoations annotations &lt;- tibble( label = c(&quot;No association&quot;, &quot;**Negative association**&quot;, &quot;No association&quot;), gdp_per_cap = c(1300, 10000, 80000), prop_plastic_waste_misman = c(0.98, 0.98, 0.98) ) # specify boundaries of shaded background layer group_boundaries &lt;- c(3000, 35000) # create the plot p &lt;- waste_mismanaged_gdp_plot %&gt;% # create the base for the plot ggplot(mapping = aes(x = gdp_per_cap, y = prop_plastic_waste_misman)) + # add background colour which emphasis three potential trends # dependent of gdp_per_capita geom_rect(aes(xmin = group_boundaries[1], xmax = group_boundaries[2], ymin = -0.05, ymax = 1.05), fill = bg_colour, alpha = 0.1) + # add observations from source data geom_point(alpha = 0.8, colour = &quot;grey40&quot;, size = 1.5) + # adjust scales to (hopefully) make the plot easier to interpret scale_x_log10(labels = scales::dollar_format(accuracy = 2)) + scale_y_continuous(labels = scales::percent) + scale_fill_viridis_c(option = &#39;magma&#39;, direction = -1, end = 0.85, begin = 0.15) + # remove unneccesary legends which might make the plot more confusing to look at guides(size = FALSE, fill = FALSE) + # adjust y axis slightly to accomadate annotations coord_cartesian(ylim = c(0,1)) + # add plot labels labs(title = glue::glue(&quot;Is an increase in GDP per capita &lt;span style = &#39;color:{text_colour};&#39;&gt;**associated**&lt;/span&gt; with a reduction&lt;br&gt;in the percentage of plastic waste a country mismanages?&lt;br&gt;&quot;), x = &quot;\\nGDP per capita\\n(on a log scale)&quot;, y = &quot;\\nPlastic waste mismanaged\\n&quot;) + # add annotations geom_richtext(data = annotations, mapping = aes(x = gdp_per_cap, y = prop_plastic_waste_misman, label = label), fill = NA, label.color = NA, colour = text_colour, size = 3) + # tweak visual appearance of the plot theme(panel.grid = element_blank(), # clean up plot background plot.title = element_markdown()) # allow text formating The first plot (below) shows GDP per capita on a log scale verses the proportion of plastic waste mismanaged. The overall relationship between the two variables is clearly non-linear. However, alternatively, I could view the plot as showing three different groups of countries each with a different linear relationships between the log of GDP per capita and the proportion of plastic waste mismanaged. The grey vertical lines in the plot below highlight where I estimated by eye the boundaries between these groups to fall. The groups themselves could be described as follows. Group 1: The lowest income countries where up to a threshold (approx. $3,000) increasing GDP per capita does not appear to associated with reductions in plastic waste mismanagement. Group 2: The middle income countries where up to another threshold (approx. $35,000) increasing GDP does appear to be associated with reductions in plastic waste mismanagement. Group 3: The highest income countries where above the second threshold (approx. $35,000) increasing GDP does not appear to be associated with further reductions in plastic waste mismanagement. These groupings could be explained in terms of countries needing to reach a certain level of economic development before the infrastructure, capabilities and political imperative to effectively manage plastic waste emerge. Beyond this level of economic development a countys increasing income and wealth allow it to reduce the proportion of plastic waste which is mismanaged. However, as the proportion of waste mismanaged gets closer to the natural limit of zero, reductions in waste mismanagement become harder to achieve. So, beyond a second level of economic development there are few further gains to made in terms of reducing plastic waste mismanagement. # display the plot p Revising the plot helps to highlight the multiple potential trends amongst middle income countries. This requires a little the original plot to be tweaked a little in the code below. # remove layers from previous plot which are not needed for revised plot p1 &lt;- p p1$layers[[3]] = NULL #p1$layers[[2]] = NULL # subset the data to approximate groups which appear to show different trends trend_1_df &lt;- waste_mismanaged_gdp_plot %&gt;% filter(gdp_per_cap &gt; group_boundaries[1] &amp; gdp_per_cap &lt; group_boundaries[2]) %&gt;% filter(prop_plastic_waste_misman &gt; 0.5) trend_2_df &lt;- waste_mismanaged_gdp_plot %&gt;% filter(gdp_per_cap &gt; group_boundaries[1] &amp; gdp_per_cap &lt; group_boundaries[2]) %&gt;% filter(prop_plastic_waste_misman &lt; 0.5) trend_3_df &lt;- waste_mismanaged_gdp_plot %&gt;% filter(gdp_per_cap &lt; group_boundaries[1]) trend_4_df &lt;- waste_mismanaged_gdp_plot %&gt;% filter(gdp_per_cap &gt; group_boundaries[2]) #&#39; Creates a linear regression line to add to a plot (given subset of the plotting data) #&#39; #&#39; @param df: a subset of the waste_mismanaged_gdp_plot dataframe #&#39; @param c: colour name or hexadecimal reference #&#39; @param ...: used to pass additional arguments to geom_line #&#39; #&#39; @return: a ggplot2 layer object add_trend_reg_line &lt;- function(df, c = plot_colour_pal[9], ...){ geom_line(data = df, mapping = aes(x = gdp_per_cap, y = prop_plastic_waste_misman), stat = &quot;smooth&quot;, method = &quot;lm&quot;, se = FALSE, colour = c, size = 1.5, ...) } # revise the plot to highlight the sub trends within the data p1 &lt;- p1 + # a linear regression lines to highlight the four trends add_trend_reg_line(trend_1_df) + add_trend_reg_line(trend_2_df) + # add labels and annotations labs(title = &quot;Potential sub-groups of &#39;middle&#39; income countries\\n&quot;) + annotate(&quot;text&quot;, x = 10000, y = 0.95, label = &quot;\\&#39;Middle\\&#39; income\\n countries&quot;, size = 3.5) + # apply additional plot styling theme_light() # add back in grid lines a useful reference in this version Re-plotting highlights that there are two (or potentially three) sub-group of interest within the group of middle income countries. Indicating the potential for an interaction between gdp_per_cap and another as yet unidentified (categorical) predictor. I did some quick exploratory plotting with the data to hand and was not able to identify a good candidate for the additional predictor. So, I leave this potential additional predictor and the interaction aside for now and proceed to start the modeling section of this notebook. # replot p1 3.3 Modelling: Piecewise linear regression 3.3.1 Fitting the model Based on the exploratory data analysis above, I decide to create an initial model using piecewise linear regression. This simply involves partitioning the dataset into non-overlapping groups (based on values of predictor variable) and fitting a linear regression model separately to each group. Based on the scatter plots above I also considered fitting a polynomial regression model. However, I went with piece-wise linear regression with hope that it would produce a model where the regression coefficients are easier to interpret (than in a polynomial model). Additionally, working with the piece-wise approach presented an interesting opportunity to apply some function programming techniques I have been learning recently (but Ill return to the practicalities of implementation in the code comments below). The first stage in piecewise linear regression modeling is to partition data. In the exploratory data analysis I estimated by eye the boundaries for partitioning based on gdp_per_cap. In the modeling I want to apply a hopefully more rigorous approach to identifying the boundaries. So, in the code below I use a piecewise regression algorithm from the segmented package to estimate the boundaries (referred to as breakpoints in the algorithm output below). Note, that in the output below the breakpoints are on a log transformed scale. # there were difficulties using the log transformation in the formula # when estimating breakpoints # so I the data needed preprocessing waste_mismanaged_gdp_mod &lt;- waste_mismanaged_gdp_plot %&gt;% mutate(gdp_per_cap = log(gdp_per_cap)) # stanglm objects are not compatible with the estimation algorithm # so I used lm instead lm_1 &lt;- lm(formula = prop_plastic_waste_misman ~ gdp_per_cap, data = waste_mismanaged_gdp_mod) # run algorithm to estimate break points segmented::segmented(lm_1, seg.Z = ~ gdp_per_cap, # starting points for the break point estimation algorithm # indentified by eye from the plot above psi = c(8.0, 10.5)) ## Call: segmented.lm(obj = lm_1, seg.Z = ~gdp_per_cap, psi = c(8, 10.5)) ## ## Meaningful coefficients of the linear terms: ## (Intercept) gdp_per_cap U1.gdp_per_cap U2.gdp_per_cap ## 1.13980 -0.04269 -0.35059 0.35316 ## ## Estimated Break-Point(s): ## psi1.gdp_per_cap psi2.gdp_per_cap ## 8.409 10.241 Having defined the boundaries for partitioning the dataset into three groups, the next step was to run the same linear regression modelling workflow for each group. I took this as an opportunity to apply some functional programming techniques I been learning over recent months. See comments in the code below for comments on the implementation. # define boundardies (on gdp_per_capita) for piecewise regression gdp_bound_1 &lt;- 4500 gdp_bound_2 &lt;- 27000 # create a new variable to identify which of the three groups an observation belongs to segmented_data &lt;- waste_mismanaged_gdp_mod %&gt;% mutate(gdp_per_cap = exp(gdp_per_cap), income_group = case_when( gdp_per_cap &lt; gdp_bound_1 ~ 1, gdp_per_cap &lt; gdp_bound_2 ~ 2, TRUE ~ 3 )) # create an indivudal data frame for each group of observations nested_segmented_data &lt;- segmented_data %&gt;% group_by(income_group) %&gt;% nest() %&gt;% rename(lm_in = data) # run the modelling workflow for each group of observationa segmented_models &lt;- nested_segmented_data %&gt;% mutate( # independently fit a linear regression model to get of the three groups stan_glm = map(.x = lm_in, .f = ~ stan_glm( formula = prop_plastic_waste_misman ~ log(gdp_per_cap), data = .x, refresh = 0)), # extract regression coefficients for each group (used in analysis below) coef = map(.x = stan_glm, .f = ~ broom.mixed::tidy(coef(.x))), # ahead of using the model to predice create a data grid of predictors points # for each group data_grid = map(.x = lm_in, .f = ~ data_grid( .x, gdp_per_cap = seq_range(gdp_per_cap, n = 100)) ), # for each predictor point in the data grid get a dataframe of draws from # the posterior predictive distriubtion # used for calculating the creditibilty interval shown in the plot below data_grid = map2(.x = data_grid, .y = stan_glm, .f = ~ add_predicted_draws(.x, .y)), bayes_r2 = map(.x = stan_glm, ~ as_tibble(bayes_R2(.x))) ) %&gt;% # re-arrange to ensure plot facets and table of coefficients are in # in a sensible order arrange(income_group) Then I constructed the plot itself. # focus in on the draws from posterior predictive (for each group of countries) plotting_df &lt;- segmented_models %&gt;% select(-stan_glm, -lm_in, -coef) %&gt;% unnest(data_grid) # define readable labels for the plot facets facet_labs &lt;- c(&quot;\\&#39;Lower\\&#39; income countries&quot;, &quot;\\&#39;Middle\\&#39; income countries&quot;, &quot;\\&#39;Higher\\&#39; income countries&quot;) names(facet_labs) &lt;- c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;) # identify which countries (lying outside the credbility interval) # should be labeled country_labels &lt;- segmented_data %&gt;% filter( (income_group == 1 &amp; prop_plastic_waste_misman &lt; 0.5 ) | (income_group == 3 &amp; prop_plastic_waste_misman &gt; 0.2) | (income_group == 2 &amp; gdp_per_cap &gt; 13000 &amp; prop_plastic_waste_misman &gt; 0.74)| (income_group == 2 &amp; gdp_per_cap &gt; 20000 &amp; prop_plastic_waste_misman &gt; 0.55) ) # create plot p &lt;- plotting_df %&gt;% # plot base ggplot(aes(gdp_per_cap, `.prediction`)) + # add credibility interval stat_lineribbon(.width = c(.95), alpha = 0.5, colour = &quot;grey10&quot;, fill = plot_colour_pal[5]) + # add observed data geom_point(data = segmented_data, mapping = aes(gdp_per_cap, prop_plastic_waste_misman), colour = &quot;grey10&quot;, alpha = 0.5) + # format axis scale_x_log10(labels = scales::dollar_format(accuracy = 2)) + scale_y_continuous(breaks = seq(0,1,0.2), labels = scales::percent) + # seperate each section of the piecewise model # to make the plot easier to interpret facet_wrap(~income_group, scales = &quot;free_x&quot;, labeller = labeller(income_group = facet_labs)) + # label points lying outside the credibility interval ggrepel::geom_text_repel(data = country_labels, mapping = aes(gdp_per_cap, prop_plastic_waste_misman, label = country), min.segment.length = 0.1, nudge_y = 0.1, size = 3.5) + # adjust y axis to omit area of the crediblity interval which fall outside # plausible values (i.e. outside ther ange 0-100%) coord_cartesian(ylim = c(0,1)) + # add plot label labs(x = &quot;\\nGDP per capita&quot;, y = &quot;\\nPlastic waste mismanaged\\n&quot;, fill = &quot;Credibility interval&quot;, subtitle = &quot;&quot;) + # remove legend which is visual clutter and doesn&#39;t aid plot interpretation # in this case guides(size = FALSE) + # remove grid lines to focus attention on credibility intervals theme(panel.grid = element_blank()) The plot below shows a 95% (Bayesian) credibility interval for the regression model. This corresponds to an expectation that approximately 95% of model predictions will fall within the credibility interval. Although we see almost all of the observed data falls within the credibility intervals, the plot also highlights some potential issues with the model fit. In places the credibility interval are over 50 percentage points wide. When of course observation can only fall in the range between 0 and 100% of plastic waste being mismanged. At least for middle income countries this reflects this seems to reflect to some extent the variance of the observation. Which in turn of course suggest that including other predictors would potentially improve the model fit. The credibility interval, and hence samples from the posterior predictive which underpin these intervals, extend beyond the possible observable range for percentage of plastic waste mismanaged. # choose an interactive or static plot # interactive plot was used during analysis to hover over and indentify points # outside the credibility intervals interactive_plot &lt;- FALSE if (interactive_plot) { plotly::ggplotly(p) } else { p + labs( title = glue::glue( &quot;95% of model prediction are likely to fall within this &lt;span style = &#39;color:{plot_colour_pal[8]};&#39;&gt;credibility interval&lt;/span&gt;&quot;), subtitle = &quot;points show the observed data\\n&quot;) + theme(plot.title = element_markdown()) } 3.3.2 Interpreting the model coefficients Having fitted the model and used the posterior predictive distribution to show the uncertainty in the models prediction, I wanted to look at the model coefficients in more detail. The first step was to extract and combine the coefficients for each of the three models that make up the overall piecewise regression. # get the coefficients for the three models summary_table_1 &lt;- segmented_models %&gt;% select(-stan_glm, -lm_in, -data_grid, -bayes_r2) %&gt;% unnest(coef) # format table summary_table_1 %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_minimal() income_group names x 1 (Intercept) 1.1385232 1 log(gdp_per_cap) -0.0424019 2 (Intercept) 4.0550296 2 log(gdp_per_cap) -0.3895086 3 (Intercept) 0.4349172 3 log(gdp_per_cap) -0.0368891 Based on the coefficients in the table above, the overall piecewise model can be expressed in mathematical notation as follows. \\[ y_{i} = \\begin{cases} 1.13 - 0.04\\ln x_{i} + \\epsilon_{1} &amp;\\text{if } x_{i} &lt; 4500 \\\\ 3.99 - 0.38\\ln x_{i} + \\epsilon_{2} &amp;\\text{if } 4500 \\leq x_{i} &lt; 27000 \\\\ 0.55 - 0.05\\ln x_{i} + \\epsilon_{3} &amp;\\text{if } x_{i} \\geq 27000 \\end{cases} \\] Where: \\(y_{i}\\) is the estimated value of the proportion of plastic waste mismanaged for the ith point; and, \\(x_{i}\\) is the value of gdp per capita for the ith point. Interpreting the coefficents of \\(x_{i}\\) is relatively straight-forward: For lower income countries (\\(x_{i} &lt; 4500\\)): a one percent increase in gdp_per_cap is (approximately) associated with a reduction of 0.04 percentage points in the percentage of plastic waste mismanaged. For middle income countries (\\(4500 \\leq x_{i} &lt; 27000\\)): a one percent increase in gdp_per_cap is (approximately) associated with a reduction of 0.38 percentage points in the percentage of plastic waste mismanaged. For higher income countries (\\(x_{i} \\geq 27000\\)): a one percent increase in gdp_per_cap is (approximately) associated with a reduction of 0.05 percentage points in the percentage of plastic waste mismanaged. However, directly interpreting the intercept terms is more challenging. First, this is because the intercept terms correspond to the expected proportion of plastic waste mismanaged for a country with zero GDP per capita. And, of course such a country does not and arguably could not exist. Secondly, the intercept terms for low and middle income countries do not lie within the range 0 to 1 (i.e. the possible values for a proportion). In general, these type of challenges with interpreting intercept terms can be addressed by subtracting an appropriate offset (often the mean or median) from the predictor values so the intercept term corresponds to the expected value of y for a sensible value of x. However, in this case due to the piecewise nature of the model offset would complicate interpretation further. 3.3.3 Some quick diagnostics Finally, ahead of considering what steps would be need to develop the model further, I wanted to do a more formal (but very quick) assessment of how well the model performs in estimating the proportion of plastic waste mismanaged for in sample data. # convenience function for mapping median to a specific dataframe calc_r2_median &lt;- function(df){ return(median(df$value)) } # convenience function for mapping median to a specific dataframe calc_sd_obs &lt;- function(df){ return(sd(df$prop_plastic_waste_misman, na.rm = TRUE)) } summary_table_2 &lt;- segmented_models %&gt;% # focus on columns needed to produce summary statistics select(income_group, lm_in, bayes_r2, stan_glm) %&gt;% # calculate summary stats for each group of countries / segment of the model mutate(r2 = map_dbl(bayes_r2, ~ calc_r2_median(.x)), plastic_misman_sd_obs = map_dbl(lm_in, ~ calc_sd_obs(.x)), abs_residuals_mean = map_dbl(stan_glm, ~mean(abs(residuals(.x))))) %&gt;% # focus on columns needed to convey the key information select(-lm_in, -bayes_r2, -stan_glm) %&gt;% # add text description of country groups to aid interpretation of the table add_column(income_group_desc = c(&quot;\\&#39;Lower\\&#39; income countries&quot;, &quot;\\&#39;Middle\\&#39; income countries&quot;, &quot;\\&#39;Higher\\&#39; income countries&quot;), .before = &quot;r2&quot;) #mean(abs(residuals(segmented_models$stan_glm[[2]]))) The table below shows the Bayesian R2 values (bayes_r2 - the proportion of variance explained by the model) and the mean of the absolute values of the residuals for each group of countries. For context, the standard deviation of the proportion of plastic waste mismanaged in the observed data is shown in the plastic_misman_sd_obs column. Clearly overall the model is not performing particularly well. The R2 values for the lower and higher income groups of countries both being below 0.05. Basically, gdp_per-cap explains almost none of the variability in the outcome variable in these group. However, it should be noted there is not much variability of the proportion of plastic waste mismanaged in the observed data (as shown in the plastic_misman_sd_obs column), and the absolute values of the residuals tend to be relatively small. The R2 value for the middle income group of countries is higher at approximately 0.46. This of course means that the proportion of variance unexplained by the model is 0.54. While the absolute values of the residuals tend to be relatively large. Both these factors highlight the need to further develop the model to improve its ability to estimate the proportion of plastic waste a country mismanages. summary_table_2 %&gt;% kableExtra::kable() %&gt;% kableExtra::kable_minimal() income_group income_group_desc r2 plastic_misman_sd_obs abs_residuals_mean 1 Lower income countries 0.0356263 0.1137020 0.0640732 2 Middle income countries 0.4598803 0.2520905 0.1553732 3 Higher income countries 0.0426978 0.0613529 0.0336369 3.4 Reflections and next steps In this notebook I developed a simple model developed above for estimating the proportion of plastic waste a country mismanages, based on its GDP per capita. Comparing the observed values with the models estimates of the proportion of plastic waste a country mismanages showed that for many countries the model estimates are disappointingly inaccurate. Nevertheless, the model developed in this notebook represents a first step towards developing a more complex and potential more accurate model. As and when I return to further develop the model my next steps would include: Exploring additional data sources to identify additional potential predictor variables (such as metrics might give an indication of the maturity of countrys disposal system or its waste disposal culture). Engineering the feature of model input data including iteratively integrating additional predictor variables and interactions into the model. Comparing the fit piecewise model in this notebook with a model using interactions between gdp_per_capita and a variable gdp_per_capita_group (which partitions countries in to one of three groups). "],["predicting-course-evaluation-scores.html", "Chapter 4 Predicting course evaluation scores 4.1 Data importing and cleaning 4.2 Exploratory data analysis 4.3 Fitting the model", " Chapter 4 Predicting course evaluation scores # import packages used in this notebook suppressPackageStartupMessages({ suppressWarnings({ # core libraries library(tidyverse) library(tidymodels) library(rstanarm) # helper libraries library(janitor) # for cleaning names library(skimr) # for summary statistic reports library(glue) # for string formatting library(infer) # for exploratory inference library(patchwork) # for displaying multiple plots together library(corrr) # for correlation analysis library(broom.mixed)# for tidying stan objects }) }) # global setting for notebook theme_set(theme_light()) # import my own plotting functions for styling plots source(&quot;plotting_functions.R&quot;) # import my own functions for producing summary stats (which use on skimr) source(&quot;stats_functions.R&quot;) 4.1 Data importing and cleaning 4.1.1 Importing the data The data I am using in this project is available in the github repository for the book Regression and Other Stories. So, I can read in the csv directly from the repository. # set url for dataset data_url &lt;- &quot;https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Beauty/data/ProfEvaltnsBeautyPublic.csv&quot; # read data from github teaching_eval &lt;- read_csv(data_url) %&gt;% janitor::clean_names() # try to clean up names # display dataframe for initial inspection glimpse(teaching_eval) ## Rows: 463 ## Columns: 64 ## $ tenured &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0~ ## $ profnumber &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1~ ## $ minority &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0~ ## $ age &lt;dbl&gt; 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37, 42, ~ ## $ beautyf2upper &lt;dbl&gt; 6, 2, 5, 4, 9, 5, 5, 6, 5, 6, 4, 5, 5, 5, 7, 3, 1, 6~ ## $ beautyflowerdiv &lt;dbl&gt; 5, 4, 5, 2, 7, 6, 4, 4, 3, 5, 5, 4, 4, 3, 6, 2, 3, 3~ ## $ beautyfupperdiv &lt;dbl&gt; 7, 4, 2, 5, 9, 6, 4, 6, 7, 7, 7, 5, 7, 3, 7, 4, 4, 5~ ## $ beautym2upper &lt;dbl&gt; 6, 3, 3, 2, 6, 6, 4, 3, 5, 6, 7, 4, 5, 5, 4, 7, 3, 4~ ## $ beautymlowerdiv &lt;dbl&gt; 2, 2, 2, 3, 7, 5, 4, 2, 5, 3, 2, 3, 4, 1, 5, 5, 2, 4~ ## $ beautymupperdiv &lt;dbl&gt; 4, 3, 3, 3, 6, 5, 4, 3, 3, 6, 4, 5, 4, 7, 4, 4, 2, 4~ ## $ btystdave &lt;dbl&gt; 0.2015666, -0.8260813, -0.6603327, -0.7663125, 1.421~ ## $ btystdf2u &lt;dbl&gt; 0.2893519, -1.6193560, -0.1878249, -0.6650018, 1.720~ ## $ btystdfl &lt;dbl&gt; 0.4580018, -0.0735065, 0.4580018, -1.1365230, 1.5210~ ## $ btystdfu &lt;dbl&gt; 0.8758139, -0.5770065, -1.5455530, -0.0927330, 1.844~ ## $ btystdm2u &lt;dbl&gt; 0.6817153, -1.1319040, -1.1319040, -1.7364440, 0.681~ ## $ btystdml &lt;dbl&gt; -0.9000649, -0.9000649, -0.9000649, -0.3125226, 2.03~ ## $ btystdmu &lt;dbl&gt; -0.1954181, -0.6546507, -0.6546507, -0.6546507, 0.72~ ## $ class1 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class2 &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class3 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class4 &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0~ ## $ class5 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class6 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class7 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class8 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class9 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0~ ## $ class10 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0~ ## $ class11 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class12 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class13 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class14 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class15 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class16 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class17 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class18 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class19 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class20 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class21 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class22 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class23 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class24 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class25 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class26 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class27 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class28 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class29 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ class30 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ courseevaluation &lt;dbl&gt; 4.3, 4.5, 3.7, 4.3, 4.4, 4.2, 4.0, 3.4, 4.5, 3.9, 3.~ ## $ didevaluation &lt;dbl&gt; 24, 17, 55, 40, 42, 182, 33, 25, 48, 16, 18, 30, 28,~ ## $ female &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0~ ## $ formal &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ fulldept &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1~ ## $ lower &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1~ ## $ multipleclass &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0~ ## $ nonenglish &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0~ ## $ onecredit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ percentevaluating &lt;dbl&gt; 55.81395, 85.00000, 100.00000, 86.95652, 87.50000, 6~ ## $ profevaluation &lt;dbl&gt; 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.0, 3.~ ## $ students &lt;dbl&gt; 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34, 40,~ ## $ tenuretrack &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0~ ## $ blkandwhite &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0~ ## $ btystdvariance &lt;dbl&gt; 2.1298060, 1.3860810, 2.5374350, 1.7605770, 1.693100~ ## $ btystdavepos &lt;dbl&gt; 0.201567, 0.000000, 0.000000, 0.000000, 1.421450, 0.~ ## $ btystdaveneg &lt;dbl&gt; 0.000000, -0.826081, -0.660333, -0.766312, 0.000000,~ Next I focused down on variables within the dataset to be used in the exploratory data analysis and modeling below. I reviewed the data code book to identify the outcome variable and promising potential predictor variables. A number of variables have been dropped in the process of focusing down the dataset , as I thought these dropped variable were unlikely to important predictors of course_evaluation scores. This was a judgment based on my experiences of working in various universities over a decade or so. Due to the nature of the variable names in the dataset, automatically cleaning all variables names to make readable and easy to interpret was not possible. So, in the process of focusing down the dataset I renamed various variables to make them easier to remember, interpret and work with. I also reorder the variables to group them in a more logical order: an outcome variable, followed by potential predictors relating to the professor, and then potential predictors relating to the class. # focus down on features in the dataset with a good potential to act as predictors teaching_eval_focus &lt;- teaching_eval %&gt;% select( # select and rename outcome variable course_evaluation = courseevaluation, # select and rename potential predictor variables relating to the professor tenured:age, prof_ave_rating = profevaluation, prof_ave_beauty_rating = btystdave, female, formal, non_native_english = nonenglish, tenure_track = tenuretrack, # select and rename potential predictor relating relating to the course lower, multiple_prof = multipleclass, one_credit = onecredit, eval_response_rate = percentevaluating, students, class1:class30 # dummy variables ) %&gt;% # Move a potential indentifer variable to the front of the dataframe # and rename for consistency rename(prof_number = profnumber) %&gt;% relocate(prof_number, .before = course_evaluation) teaching_eval_focus ## # A tibble: 463 x 46 ## prof_number course_evaluation tenured minority age prof_ave_rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 4.3 0 1 36 4.7 ## 2 2 4.5 1 0 59 4.6 ## 3 3 3.7 1 0 51 4.1 ## 4 4 4.3 1 0 40 4.5 ## 5 5 4.4 0 0 31 4.8 ## 6 6 4.2 1 0 62 4.4 ## 7 7 4 0 0 33 4.4 ## 8 8 3.4 1 0 51 3.4 ## 9 9 4.5 0 0 33 4.8 ## 10 10 3.9 0 0 47 4 ## # ... with 453 more rows, and 40 more variables: prof_ave_beauty_rating &lt;dbl&gt;, ## # female &lt;dbl&gt;, formal &lt;dbl&gt;, non_native_english &lt;dbl&gt;, tenure_track &lt;dbl&gt;, ## # lower &lt;dbl&gt;, multiple_prof &lt;dbl&gt;, one_credit &lt;dbl&gt;, ## # eval_response_rate &lt;dbl&gt;, students &lt;dbl&gt;, class1 &lt;dbl&gt;, class2 &lt;dbl&gt;, ## # class3 &lt;dbl&gt;, class4 &lt;dbl&gt;, class5 &lt;dbl&gt;, class6 &lt;dbl&gt;, class7 &lt;dbl&gt;, ## # class8 &lt;dbl&gt;, class9 &lt;dbl&gt;, class10 &lt;dbl&gt;, class11 &lt;dbl&gt;, class12 &lt;dbl&gt;, ## # class13 &lt;dbl&gt;, class14 &lt;dbl&gt;, class15 &lt;dbl&gt;, class16 &lt;dbl&gt;, class17 &lt;dbl&gt;, ## # class18 &lt;dbl&gt;, class19 &lt;dbl&gt;, class20 &lt;dbl&gt;, class21 &lt;dbl&gt;, class22 &lt;dbl&gt;, ## # class23 &lt;dbl&gt;, class24 &lt;dbl&gt;, class25 &lt;dbl&gt;, class26 &lt;dbl&gt;, class27 &lt;dbl&gt;, ## # class28 &lt;dbl&gt;, class29 &lt;dbl&gt;, class30 &lt;dbl&gt; 4.1.2 Checking for missing values Next I checked for missing data. While importing the data I notice there were 30 dummy variables used to identify the class being evaluated. I wanted to look in a bit more detail at potential missing values across the dummy variables. So, first I removed the dummy variables and looked at all the other variables. From the missing values check below it looks like this dataset has already to processed as not NAs have been identified. visdat::vis_miss(teaching_eval_focus %&gt;% select(-starts_with(&quot;class&quot;))) At first glance the there are no NAs identified for the dummy variables, for all pairs of observations of observations and dummy variables either a 1 or a 0 is recorded. visdat::vis_miss(teaching_eval_focus %&gt;% select(starts_with(&quot;class&quot;))) However, if looking at these dummy variables in more detail it is clear that a lot of data is missing. By summing the all the dummy variables for a given observation, we can see if any classes are recorded for each observation. I would expect this to be 1 where a class is recorded and 0 where a class is not recorded. Hence, in this context a 0 actually corresponds to a missing value and in the plot below we can see that for approximately two thirds of observations a class is not recorded. # check if each evaluation has a class (1-30) associated with it # by processing the dummy variables dummy_var_row_sum &lt;- teaching_eval_focus %&gt;% select(class1:class30) %&gt;% rowSums() %&gt;% as_tibble() %&gt;% rename(class_indentified = value) # not all evaluations have a class # associated with them visdat::vis_miss(dummy_var_row_sum %&gt;% na_if(0)) Working with the dummy variables in the analysis and modeling below would entail dropping two thirds observations and n = 463. So, although I recognized the course identifier dummy variables as potentially important predictors of course_evaluation I dropped them from the analysis, rather than loose so many observations from the dataset. teaching_eval_clean &lt;- teaching_eval_focus %&gt;% select(-starts_with(&quot;class&quot;)) glimpse(teaching_eval_clean) ## Rows: 463 ## Columns: 16 ## $ prof_number &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, ~ ## $ course_evaluation &lt;dbl&gt; 4.3, 4.5, 3.7, 4.3, 4.4, 4.2, 4.0, 3.4, 4.5, 3.~ ## $ tenured &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,~ ## $ minority &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,~ ## $ age &lt;dbl&gt; 36, 59, 51, 40, 31, 62, 33, 51, 33, 47, 35, 37,~ ## $ prof_ave_rating &lt;dbl&gt; 4.7, 4.6, 4.1, 4.5, 4.8, 4.4, 4.4, 3.4, 4.8, 4.~ ## $ prof_ave_beauty_rating &lt;dbl&gt; 0.2015666, -0.8260813, -0.6603327, -0.7663125, ~ ## $ female &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0,~ ## $ formal &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ ## $ non_native_english &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,~ ## $ tenure_track &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,~ ## $ lower &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ ## $ multiple_prof &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1,~ ## $ one_credit &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,~ ## $ eval_response_rate &lt;dbl&gt; 55.81395, 85.00000, 100.00000, 86.95652, 87.500~ ## $ students &lt;dbl&gt; 43, 20, 55, 46, 48, 282, 41, 41, 60, 19, 25, 34~ 4.2 Exploratory data analysis 4.2.1 The response variable So, I started the exploratory data analysis quickly looking at summary statistics for course_evaluation, which is the response variable in this exercise. This first things I noted was that the both the mean and median course_evaluation score is 4, and the mini-histogram indicates a potential left skew of the data. So, it appears that the course_evaluation scores tend to be towards the higher end of the 5 point scale. I want to look at this in a little more detail, so moved on to plotting a full histogram. skim_minimal(df = teaching_eval_clean, course_evaluation) Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p50 p100 hist course_evaluation 0 1 4 0.55 2.1 4 5  num_evals &lt;- nrow(teaching_eval_clean) num_evals_three_plus &lt;- nrow( teaching_eval_clean[teaching_eval_clean$course_evaluation &gt;= 3,] ) perc_evals_three_plus &lt;- round((num_evals_three_plus / num_evals) * 100, 0) anno_text &lt;- glue::glue(&quot;{perc_evals_three_plus}% of courses have an average evaluation score of 3 or above&quot;) # create plot base p &lt;- ggplot(data = teaching_eval_clean, mapping = aes(x = course_evaluation)) + # add a reference line geom_vline(xintercept = 3, colour = &quot;grey70&quot;, size = 1.5, linetype = &quot;longdash&quot;) + geom_rect(xmin = 3, xmax = 5.05, ymin = 0 , ymax = 40, fill = &quot;grey92&quot;, alpha = 0.05) + # add annotation geom_label(x = 3.2, y = 35, label = anno_text, size = 3, colour = &quot;grey20&quot;) + # create histogram with minimal styling geom_minimal_hist(ylim=c(0,40), binwidth = 0.1) + # adjust x axis labels for readability scale_x_continuous(breaks = seq(0,5,0.5)) + expand_limits(x = 0) + # adjust y axis labels for readability scale_y_continuous(breaks = seq(0,40,10)) + # add plot labels labs(title = &quot;Course evaluation scores tend to be towards\\nthe top end of the five point scale\\n&quot;, x = &quot;\\nAverage course evaluation score\\n(given by students)&quot;, y = &quot;Number of courses\\n&quot;) So, it full plot there is some indication of left skew, in the form of the longer tail to the left of the plot. Also, evident is the fact there is a hard limit (of 5) on the maximum course_evaluation score, which accentuates the impression of the skew. p ggsave(&quot;test.svg&quot;) So, I took a quick look at course_evaluation centered around the mean. This looks like a close enough approximation to the normal distribution to me. So, ahead of modeling I dont need to think about transforming course_evaluation scores to address the limited amount of skew evident in the plot. However, it might be worth centering the course_evaluation scores as this might help make the regression coefficients easier to interpret. But Ill come back to that later on. course_evaluation_mean = mean(teaching_eval_clean$course_evaluation) p &lt;- ggplot(data = teaching_eval_clean, mapping = aes(x = course_evaluation - course_evaluation_mean)) + geom_minimal_hist(ylim=c(0,40), binwidth = 0.1) p + labs(title = &quot;Centering course evaluation scores makes it\\neasier to see the extent of skew\\n&quot;, x = &quot;\\nCentred average course evaluation score&quot;, y = &quot;Number of courses\\n&quot;) 4.2.2 Potential explanatory variables So, I moved on to looking at the potential explanatory variables within the dataset. Remembering that when I imported the data I had thought about the potential explanatory variables as belonging to two distinct groups: (1) Professor related variables; and, (2) course related variables. So, below I take at look at these two groups of variables in turn. 4.2.2.1 Professor related explanatory variables The first thing I noticed when looking at the summary statistics for the Professor related variables was that all the variables are numeric. But that there is categorical data present which has already coded as binary variables. So, I decided to split the variables again into groups of binary variables and continuous variables, before looking at the data in more detail. # focus on variables relating to professors explanatory_variables_prof &lt;- teaching_eval_clean %&gt;% select(prof_number, tenured:tenure_track) # produce summary statistics skim_minimal(explanatory_variables_prof %&gt;% select(-prof_number), # no need to include identifer variable show_data_completeness = FALSE) Variable type: numeric skim_variable mean sd p0 p50 p100 hist tenured 0.55 0.50 0.00 1.00 1.00  minority 0.14 0.35 0.00 0.00 1.00  age 48.37 9.80 29.00 48.00 73.00  prof_ave_rating 4.17 0.54 2.30 4.30 5.00  prof_ave_beauty_rating -0.09 0.79 -1.54 -0.16 1.88  female 0.42 0.49 0.00 0.00 1.00  formal 0.17 0.37 0.00 0.00 1.00  non_native_english 0.06 0.24 0.00 0.00 1.00  tenure_track 0.78 0.41 0.00 1.00 1.00  To do this I wrote a quick function to identify binary variables within the dataset. #&#39; Checks if a column is a binary variable #&#39; i.e. checks if a column only contains zeros and ones #&#39; #&#39; @param col: A column of dataframe passed by a TidyVerse function #&#39; #&#39; @return: A boolean is_binary_var &lt;- function(col){ # identify unique entries in column u &lt;- col %&gt;% unique() # check if only 0s and 1s a are within the column and return as appropriate if (identical(sort(u), c(0,1))) return(TRUE) else return(FALSE) } 4.2.2.1.1 Continuous variables There are only three continuous variables relating to Professors: the average rating given by students to each Professor teaching the course (prof_ave_rating); the average beauty rating by students to each Professor teaching the course (prof_ave_beauty_rating); and, age. Immediately below are the summary statistics for each of the three variables. I then take a more detailed look at each one of these variables in turn. # identify continous variables within the dataset explanatory_variables_prof_cont &lt;- explanatory_variables_prof %&gt;% select(where(~!is_binary_var(.))) # show summary statistics skim_minimal(explanatory_variables_prof_cont %&gt;% select(-prof_number), show_data_completeness = FALSE) Variable type: numeric skim_variable mean sd p0 p50 p100 hist age 48.37 9.80 29.00 48.00 73.00  prof_ave_rating 4.17 0.54 2.30 4.30 5.00  prof_ave_beauty_rating -0.09 0.79 -1.54 -0.16 1.88  At first glance the distribution of prof_ave_rating is very similar to the distribution of the response variable (course_evaluation) . plot_title &lt;- &quot;(Similarly to course evaluation scores)\\nProfessor rating tends to be towards the upper end of the five point scale\\n&quot; ggplot(data = explanatory_variables_prof_cont, mapping = aes(x = prof_ave_rating)) + geom_minimal_hist(ylim = c(0,45), binwidth = 0.1) + expand_limits(x = 0)+ labs(title = plot_title, x = &quot;Professor rating&quot;, y = &quot;Number of ratings\\n&quot;) The plot below compares course_evaluation and prof_ave_rating at each observation in the dataset. This shows that for a given course-professor pair the prof_ave_rating tends to be higher than the course_evaluation score. p &lt;- teaching_eval_clean %&gt;% # focus on the two variables to be comparted for each observation select(course_evaluation, prof_ave_rating) %&gt;% arrange(prof_ave_rating) %&gt;% # put the data in a long format for plotting mutate(id = 1:nrow(.), .before = course_evaluation) %&gt;% pivot_longer(cols = c(course_evaluation, prof_ave_rating), names_to = &quot;score_type&quot;, values_to = &quot;score&quot;) %&gt;% # create plot base ggplot(mapping = aes(x = score, y = id)) + # add points for prof rating and course evaluation # and connect each pair of points with a line geom_line(aes(group = id), colour = &quot;grey50&quot;) + geom_point(aes(colour = score_type)) + # show full x axis expand_limits(x = 0) + # remove gap between x axis and plot coord_cartesian(ylim = c(0, nrow(teaching_eval_clean)+5), xlim = c(0, 5.1), expand = FALSE) + # add plot labels labs(title = &quot;But Professor ratings tend to be higher than course evaluation scores&quot;, subtitle = &quot;Both are on the same five point scale\\n&quot;) + # format legend to remove clutter scale_color_brewer(palette = &quot;Set1&quot;, labels = c(&quot;Course evaluation score&quot;, &quot;Professor rating&quot;)) + guides(colour = guide_legend(title = NULL)) + # adjust theme to remove clutter remove_all_grid_lines() + theme(axis.title.y = element_blank(), axis.text.y = element_blank(), axis.line.y = element_blank(), legend.position = &quot;top&quot;, legend.text = element_text(size = 10)) p ggsave(&quot;test1.svg&quot;) Professors can appear multiple times in the dataset, where they have been involved in teaching multiple courses. The prof_ave_rating is the average of the ratings given by students to the professor for an individual course. The plot below shows the prof_ave_ratings for 10 randomly selected professors. So, highlighting that as expected there is some variability in the average ratings that professors received from course to course. # ensure the sample random sample of professors is take when notebook is rerun set.seed(100) explanatory_variables_prof_cont %&gt;% # select a random sample of professors group_by(prof_number) %&gt;% nest() %&gt;% ungroup() %&gt;% slice_sample(n = 10) %&gt;% mutate(prof_number = factor(prof_number)) %&gt;% # convert to factor # to simplify plotting unnest() %&gt;% # produce plot ggplot(aes(prof_ave_rating, prof_number)) + # jitter slightly to make overlapping points easier to see geom_jitter(alpha = 0.5, height = 0.1) + # adjust axis for readibility expand_limits(x = 0)+ coord_flip() + # add plot labels labs(y = &quot;Professor identification number&quot;, x = &quot;Course evaluation score&quot;, title = &quot;There is some variablility in the evaluation scores professors\\n received from course to course&quot;, subtitle = &quot;Showing course evaulation scores for 10 randomly selected professors&quot;) So, averaging ratings for a given professor over multiple courses might give an idea of the quality of teaching delivered in general by a given professor. Ill call this mean of average ratings over multiple course ave_teaching_qual just to avoid confusing terminology created with the multiple levels of averaging taking place. Ill return to this estimate of teaching quality delivered by each professor later on, as it may be a potentially useful predictor of a course_evaluation score. prof_ave_teach_qual &lt;- explanatory_variables_prof_cont %&gt;% group_by(prof_number) %&gt;% summarise(ave_teaching_qual = mean(prof_ave_rating)) explanatory_variables_prof_cont &lt;- explanatory_variables_prof_cont %&gt;% left_join(prof_ave_teach_qual) Moving on to the next professor related continuous variable the prof_ave_beauty_rating. There is a single beauty rating for each professor. I confirmed this in the code below by confirming the standard deviation of each Professors prof_ave_beauty_rating is zero. # check if the beauty of professors has been rated seperately for each course explanatory_variables_prof_cont %&gt;% group_by(prof_number) %&gt;% summarise(beauty_rating_sd = sd(prof_ave_beauty_rating)) %&gt;% # I am not sure why NAs are being produced here # I checked the dataframe manually and they look like they relate to empty rows # at the end of the dataframe. After a quick Google I could find an obvious # reason for these empty rows, so I decided to leave this issues for now. distinct(beauty_rating_sd) ## # A tibble: 2 x 1 ## beauty_rating_sd ## &lt;dbl&gt; ## 1 0 ## 2 NA Looking at the prof_ave_beauty_ratings in the plot below, it looks like again the rating were on five point scale and have already been centered. I am going to assume zero corresponds to average beauty on a Likhert scale (very unattractive, unattractive, average attractiveness, attractive, very attractive). Then it looks like there may be some tendency for students to rate their Professors as being unattractive or around average. Remembering that the ratings plotted below are averages and assuming that students were not asked to rate attractiveness on a more granular scale, where ratings of for example -0.1 could be given. A Likhert scale seems much more likely. ggplot(explanatory_variables_prof_cont, aes(prof_ave_beauty_rating)) + geom_minimal_hist(ylim = c(0,70), binwidth = 0.2) + labs(title = &quot;Do students tend to rate Professor as average\\nor below average attactiveness?&quot;, x = &quot;Average beauty rating&quot;, y = &quot;Number of professors\\n&quot;) Finally, for the Professor related continuous variables I plot the distribution of age. There is too much too note here. Other than the shape of the histogram is rather sensitive to the width of the bins. ggplot(explanatory_variables_prof_cont, aes(age)) + geom_minimal_hist(ylim = c(0,110), binwidth = 5) + labs(title = &quot;The distribution of the ages of Professors begins to approximate\\n the normal distriubtion with an appropriate binwidth&quot;) This made wonder if it might be helpful during the modelling to have a binned (i.e. discretised) age variable which groups professors by age. # create discrete age variable (5 year age groups starting at age 20) explanatory_variables_prof_cont &lt;- explanatory_variables_prof_cont %&gt;% mutate(age_binned = cut_width(age, width = 5, boundary = 20)) 4.2.2.1.2 Discrete variables Moving on to look at the discrete (primarily binary) Professor related variables. I start by looking at the summary statistics. There is not too much to do in terms of visualisation of these variables at this point. For the binary variables the mean value (shown in the summary stats below) indicates the proportion of professors with the characteristic associated with the variable (e.g. The proportion of professors who are tenured is 0.546). So, we can see that binary variables differ in the extent to which they partition the professors in to groups of equal or unequal size. # identify binary variables within the dataset explanatory_variables_prof_discrete &lt;- explanatory_variables_prof %&gt;% select(where(~is_binary_var(.))) %&gt;% # add back in identifer variable and reorder for readability bind_cols(select(explanatory_variables_prof, prof_number)) %&gt;% relocate(prof_number, .before = tenured) %&gt;% # add in age group variable group bind_cols(select(explanatory_variables_prof_cont, age_binned)) # show summary statistics skim_minimal(explanatory_variables_prof_discrete %&gt;% select(-prof_number), show_data_completeness = FALSE) Variable type: factor skim_variable ordered n_unique top_counts age_binned FALSE 10 (45: 84, (50: 74, (55: 71, (30: 63 Variable type: numeric skim_variable mean sd p0 p50 p100 hist tenured 0.55 0.50 0 1 1  minority 0.14 0.35 0 0 1  female 0.42 0.49 0 0 1  formal 0.17 0.37 0 0 1  non_native_english 0.06 0.24 0 0 1  tenure_track 0.78 0.41 0 1 1  Looking at the binary variables we might expect to some interactions between. For example, I wonder if a larger proportion of professors will be tenured if they are not female, and below I do a quick bit of exploratory inference to see if I am correct. So, H0 is that there is no difference in the proportions of female and male professor who are tenured. And, HA is that there is a difference between the proportions of female and male professor who are tenured. Below I use the infer package to test H0 (assuming a significance level of 0.05) First, calculating the observed difference in proportions; Then, generating the null distribution (i.e. the distribution of differences we could expect from a large number of samples collected using the same methods as the observed sample) using bootstrap resampling. Finally, comparing the observed difference in proportion with the null distribution. We can see on the plot that H0 can be rejected and the p-value of observed difference (approx. equal to 0) confirms this. So, in the population of Professors from which the current observations were sampled from we can be 95% confident that there is a difference between the proportions of female and male professor who are tenured. And, given the observed difference in proportions 0.27 (male - female) we would expect more male than female professors to be tenured. # calculating the observed difference in proportion of men and women # with tenure diff_tenured &lt;- explanatory_variables_prof_discrete %&gt;% # the test requires catergorical data to be coded as a factor mutate(tenured = factor(tenured), female = factor(female)) %&gt;% specify(tenured ~ female, success = &quot;1&quot;) %&gt;% calculate(stat = &quot;diff in props&quot;) # A postive difference indicates higher proportion of men tenured diff_tenured ## # A tibble: 1 x 1 ## stat ## &lt;dbl&gt; ## 1 0.271 # generate the null distribution null_distn &lt;- explanatory_variables_prof_discrete %&gt;% # the test requires catergorical data to be coded as a factor mutate(tenured = factor(tenured), female = factor(female)) %&gt;% specify(tenured ~ female, success = &quot;1&quot;) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 1000) %&gt;% calculate(stat = &quot;diff in props&quot;) # visualise distribution under null hypothesis visualize(null_distn) + shade_p_value(obs_stat = diff_tenured, direction = &quot;two-sided&quot;) + theme_light() + remove_all_grid_lines() # confirm p-value null_distn %&gt;% get_p_value(obs_stat = diff_tenured, direction = &quot;two-sided&quot;) ## # A tibble: 1 x 1 ## p_value ## &lt;dbl&gt; ## 1 0 I think Ill leave that there in terms of exploratory inference, and just assume for that there interactions between the discrete variables and come back to that issue if and when required in the modeling process. 4.2.2.2 Course related explanatory variables Having focused on the professor related variables above, I now move on to exploring the course related variables. Similarly to the professor related variables, there are a mix of continuous and binary course related variables. So, below I look at each of two groups of variables separately. explanatory_variables_course &lt;- teaching_eval_clean %&gt;% select(lower:students) skim_minimal(explanatory_variables_course, show_data_completeness = FALSE) Variable type: numeric skim_variable mean sd p0 p50 p100 hist lower 0.34 0.47 0.00 0.00 1  multiple_prof 0.34 0.47 0.00 0.00 1  one_credit 0.06 0.23 0.00 0.00 1  eval_response_rate 74.43 16.76 10.42 76.92 100  students 55.18 75.07 8.00 29.00 581  4.2.2.2.1 Continuous variables Below I show the summary statistics below for the two continuous variables: the evaluation response rate (eval_response_rate) and the number of students taking the course (students). A quick look suggests that both the variables are heavily skewed. So, Ill look at each in a bit more details below. # identify continuous variables within the dataset explanatory_variables_course_cont &lt;- explanatory_variables_course %&gt;% select(where(~!is_binary_var(.))) explanatory_variables_course_cont %&gt;% skim_minimal(show_data_completeness = FALSE) Variable type: numeric skim_variable mean sd p0 p50 p100 hist eval_response_rate 74.43 16.76 10.42 76.92 100  students 55.18 75.07 8.00 29.00 581  Evaluation response rate: The histogram below shows that the distribution for eval_response_rate is left skewed, with the number of courses (i.e. frequency) increasing with increasing eval_response_rate up until around 80% response rate. Above 80% there is some evidence of a decline in the number of course with a given response rate. This makes sense as it would be challenging to get all the students of a given course to participate in an evaluation survey. At the moment I am not sure if it will be beneficial to transform this variable ahead of modeling. As rather than approximating a normal distribution, it is also straightforward to interpret the distribution as composed of a linear trend between 0 and 80%. And, of course the choice of bin width of the histogram plays a key role in visual appearance of distribution. Again Ill come back this later during the modelling if needed. # create plot base ggplot(explanatory_variables_course_cont, aes(eval_response_rate)) + # add the plot itself geom_minimal_hist(ylim = c(0, 40), binwidth = 2.5) + # adjust scales to make them easier to interpret scale_x_continuous(labels = scales::percent_format(scale = 1), breaks = c(0,20,40,60,80,100)) + # add more readible axis titles and a plot title labs(x = &quot;\\nStudents participating in the course evaluation survey&quot;, y = &quot;Number of courses&quot;, title = &quot;Higher participation rates in course evaluation surveys\\nare more common than lower participation rates\\n&quot;) Number of students: Moving on to look at the last course related continuous variable, below is the code to contruct the plots. #************************************************************************* # Untransformed data #************************************************************************* # create plot base p_num_stud_1 &lt;- ggplot(explanatory_variables_course_cont, aes(students)) + # add reference line geom_vline(xintercept = 100, colour = &quot;grey90&quot;, size = 2, alpha = 0.5) + # add the plot itself geom_minimal_hist(ylim = c(0, 140), binwidth = 10) + # adjust scales to make the plot easier to interpret scale_y_continuous(breaks = seq(0,140,20)) + # add more readible axis titles and a plot title labs(x = &quot;\\nNumber of students registered on a course&quot;, y = &quot;\\nNumber of courses\\n&quot;, title = &quot;Most courses have 100 students or less\\n but there are courses with much larger numbers of students\\n&quot;) #************************************************************************* # Transformed data #************************************************************************* # create plot base p_num_stud_2 &lt;- ggplot(explanatory_variables_course_cont, aes(students)) + # add reference line geom_vline(xintercept = 100, colour = &quot;grey90&quot;, size = 2, alpha = 0.8) + # add the plot itself geom_density(size = 2, colour = &quot;grey70&quot;) + scale_x_log10() + remove_all_grid_lines() + labs(x = &quot;\\nNumber of students registered on a course\\n(plotted on a log scale)&quot;, y = &quot;\\nEstimated density\\n&quot;, title = &quot;The distribution of number of students registered per course\\nremains left skewed after a log transformation \\n&quot;) The first plot shows that most courses have less than 100 students. There some course with several hundred students, I assume these outliers are first year courses which are mandatory for students taking a given degree. The distribution of the number of students registered per course is heavily left skewed, and after a log transformation some (albeit less) left skew remains. So, when it comes to pre-processing the data ahead of modeling I might take a look at other transformations, which could bring the distribution closer to approximating the normal distribution. p_num_stud_1 p_num_stud_2 4.2.2.2.2 Discrete variables The final variables to look at in isolation are the binary variables related to the courses themselves. The summary statistics below shows that approximately 5% of courses are one_credit, so I think it is unlikely to be a particularly important predictor of course_evaluation scores and I drop it from from the analysis going forward. I am happy to keep the other two binary variables in the analysis for now without looking at them in further detail at this stage. # identify continuous variables within the dataset explanatory_variables_course_discrete &lt;- explanatory_variables_course %&gt;% select(where(~is_binary_var(.))) explanatory_variables_course_discrete %&gt;% skim_minimal() Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p50 p100 hist lower 0 1 0.34 0.47 0 0 1  multiple_prof 0 1 0.34 0.47 0 0 1  one_credit 0 1 0.06 0.23 0 0 1  df &lt;- explanatory_variables_course_discrete%&gt;% skim_minimal(show_data_completeness = FALSE) # omit one_credit variable explanatory_variables_course_discrete &lt;- explanatory_variables_course_discrete %&gt;% select(-one_credit) 4.2.3 Relationships between variables Having looked in detail at each variable in isolation, I now turn to look at the relationships between the variables. 4.2.3.1 Relationships between continuous variables First looking at the correlation between each of the continuous variables. Reviewing the correlation matrix below, the key points I noted were: There is no co-linearity of concern between the continuous explanatory variables. The correlation coefficent of 0.8 between prof_ave_rating and ave_teaching_qual are to be expected given the later is calculated based on the former. This isnt of concern as I wouldnt expect to use both teaching related variables in the same model. Only prof_ave_rating and ave_teaching_qual are strongly correlated with the response variable (course_evaluation). I have concerns that prof_ave_rating is so closely correlated course_evaluation, suggesting that may be students do not delineate the quality of the professor from the quality of the course (particularly given the coarse granularity of the Likhert scales used to collect the data). # create a dataframe of all continuous variables (including the response variable) variables_all_cont &lt;- explanatory_variables_prof_cont %&gt;% bind_cols(explanatory_variables_course_cont) %&gt;% bind_cols(select(teaching_eval_clean, course_evaluation)) %&gt;% select(-prof_number, -age_binned) # produce correlation plot - default is Pearson p &lt;- GGally::ggcorr(variables_all_cont, label = TRUE) + # note key observations labs(title = &quot;There are no colinearity of concern between continuous explanatory variable&quot;, subtitle = &quot;Unsuprisngly, teaching ratings are closely correlated with course evaluation scores&quot;, caption = &quot;Pairwise Pearson&#39;s correlation coefficents are shown&quot;) + # adjust text size given plot sizes theme(plot.title = element_text(size = 18), plot.subtitle = element_text(size = 14), plot.caption = element_text(size = 12), legend.position = &quot;none&quot;) p ggsave(&quot;test2.svg&quot;) 4.2.3.2 Relationships between discrete variables and continuous variables As a final stage of the exploratory data analysis I wanted to quickly look at the relationships between the discrete and continuous predictors. In particular, I was interested in if there might be any interactions between the some of the discrete variables and the most promising predictor (ave_teaching_qual) of course_evaluation. In the code below I construct three plots to look at this. # combine the different types of variables in to a single dataframe # so, I can look at interactions interactions_plot_df &lt;- variables_all_cont %&gt;% bind_cols(explanatory_variables_course_discrete, select(explanatory_variables_prof_discrete, -prof_number)) %&gt;% select(course_evaluation, ave_teaching_qual, female, minority, tenure_track) #&#39; Plots interactions between a specified bianry variable and the #&#39; the most promising predictor (ave_teaching_qaul) of course_evaluation #&#39; #&#39; @param binary_var: a tidy select name for a column containing a binary variable #&#39; #&#39; @return: a ggplot object #&#39; plot_interaction &lt;- function(binary_var){ # create plot base p &lt;- ggplot(interactions_plot_df, aes(ave_teaching_qual, course_evaluation, colour = factor({{binary_var}})) ) + # add data layers geom_point(alpha = 0.2) + geom_smooth(se = FALSE, method = &quot;lm&quot;, size = 1.3) + # customise coloring to make plot easier to interpret scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Set1&quot;, labels = c(&quot;No&quot;, &quot;Yes&quot;)) + # so full x and y axis expand_limits(x = 0, y = 0) + # add plot labels labs(x = &quot;\\nAverage teaching quality delivered by professor&quot;, y = &quot;Course evaluation score\\n&quot;) # remove some clutter to make plot easier to interpret theme(panel.grid.minor = element_blank(), legend.position = &quot;top&quot;) return(p) } # produce and label plots for the three potential interactions of interest p_f &lt;- plot_interaction(female) + labs(colour = &quot;Female professor&quot;, x =&quot;&quot;, title = &quot;Interaction between prof gender and course evaluation score?&quot;) p_m &lt;- plot_interaction(minority) + labs(colour = &quot;Professor with\\nminority ethnic\\nbackground&quot;, x = &quot;&quot;,title = &quot;Interaction between prof background and course evaluation score?&quot;) p_tt &lt;- plot_interaction(tenure_track) + labs(colour = &quot;Professor with\\ntenure track&quot;, title = &quot;Interaction between prof tenure track status and course evaluation score?&quot;) In each of the plots below, the dataset is partitioned based on the binary variable in question, and separate linear regression lines are plotted. So, for example, the first plot show the data partitioned into two group females and males (i.e. female = Yes and female = No). Considering each of the binary variables in turn: female: there is a difference in the slopes of the regression lines for the two groups, so there could be an interaction worth exploring in the model. minority: the regression lines for the two groups look pretty much the same, so there probably is not an interaction worth exploring in the model. tenure_track: there is a difference in the slopes of the regression lines for the two groups, so there could be an interaction worth exploring in the model. # combine plots to make them easier to interpret as a group p &lt;- p_f / p_m / p_tt p ggsave(&quot;test4.svg&quot;) 4.3 Fitting the model Approach to developing the model Establish a starting point Incremental add to the model and compare using loo log score (elpd_loo) 4.3.1 Splitting the data # bring together all variables into a single data frame all_variables &lt;- select(teaching_eval_clean, course_evaluation) %&gt;% bind_cols(explanatory_variables_prof_cont, select(explanatory_variables_prof_discrete, -prof_number, - age_binned), explanatory_variables_course_cont, explanatory_variables_course_discrete) %&gt;% clean_names() all_variables ## # A tibble: 463 x 17 ## course_evaluation prof_number age prof_ave_rating prof_ave_beauty_rating ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.3 1 36 4.7 0.202 ## 2 4.5 2 59 4.6 -0.826 ## 3 3.7 3 51 4.1 -0.660 ## 4 4.3 4 40 4.5 -0.766 ## 5 4.4 5 31 4.8 1.42 ## 6 4.2 6 62 4.4 0.500 ## 7 4 7 33 4.4 -0.214 ## 8 3.4 8 51 3.4 -0.347 ## 9 4.5 9 33 4.8 0.0613 ## 10 3.9 10 47 4 0.453 ## # ... with 453 more rows, and 12 more variables: ave_teaching_qual &lt;dbl&gt;, ## # age_binned &lt;fct&gt;, tenured &lt;dbl&gt;, minority &lt;dbl&gt;, female &lt;dbl&gt;, ## # formal &lt;dbl&gt;, non_native_english &lt;dbl&gt;, tenure_track &lt;dbl&gt;, ## # eval_response_rate &lt;dbl&gt;, students &lt;dbl&gt;, lower &lt;dbl&gt;, multiple_prof &lt;dbl&gt; # %&gt;% # rename_with(.fn = ~ str_remove(., pattern = &#39;_\\\\d&#39;)) # to ensure split is reproducible set.seed(123456) # create train and test datasets teaching_split &lt;- rsample::initial_split(all_variables, prop = 0.8) teaching_train &lt;- rsample::training(teaching_split) teaching_test &lt;- rsample::testing(teaching_split) 4.3.2 Minimal and maximal models #&#39; Provides a summary of loo cross validation metrics for stan_glm model #&#39; #&#39; @param stan_lm_mod: an object with the classes &quot;stanreg&quot;, &quot;glm&quot; and &quot;lm&quot; #&#39; #&#39; @return: a tibble of metrics stan_glm_metric_summary &lt;- function(stan_lm_mod, as_kable = FALSE){ # calculate r2 metrics mod r2 &lt;- loo_R2(stan_lm_mod) # calculate loo metrics metrics &lt;- loo(stan_lm_mod)$estimates %&gt;% #reformat for ease of use data.frame() %&gt;% rownames_to_column(var = &quot;Metric&quot;) %&gt;% # add r2 to loo metrics add_row(Metric = &quot;r2&quot;, Estimate = median(r2), SE = sd(r2)) #return results if (as_kable == FALSE) return(metrics) else return(knitr::kable(metrics)) } #&#39; Compares to two stan_glm models by loo cross validation metrics #&#39; #&#39; @param mod_1: an object with the classes &quot;stanreg&quot;, &quot;glm&quot; and &quot;lm&quot; #&#39; @param mod_2: an object with the classes &quot;stanreg&quot;, &quot;glm&quot; and &quot;lm&quot; #&#39; #&#39; @return: a tibble of metrics stan_glm_loo_comp &lt;- function(mod_1, mod_2, as_kable = FALSE){ # compare the two models comp &lt;- loo_compare(loo(mod_1), loo(mod_2)) # convert comparision output to make it easier to process res &lt;- comp %&gt;% data.frame() %&gt;% # reformat output for ease of interpretation rownames_to_column(var = &quot;Model&quot;) %&gt;% pivot_longer(cols = elpd_diff:se_looic, names_to = &quot;Metric&quot;, values_to = &quot;Estimate&quot;) %&gt;% pivot_wider(names_from = &quot;Model&quot;, values_from = &quot;Estimate&quot;) %&gt;% # add in r2 metrics add_row(Metric = &quot;r2&quot;, mod_1 = median(loo_R2(mod_1)), mod_2 = median(loo_R2(mod_2))) # rename columns to the names of the objects passed to the function names(res) &lt;- c(&quot;Metric&quot;, deparse(substitute(mod_1)), deparse(substitute(mod_2))) # return results if (as_kable == FALSE) return(res) else return(knitr::kable(res)) } Minimal model: the most promising predictor identified in the EDA Maximal model: all predictors, where there are multiple predictors relating to the same underlying construct (e.g. age and age_binned) only one is retained. lm_min &lt;- stan_glm(course_evaluation ~ ave_teaching_qual, data = teaching_train, refresh = 0) lm_max &lt;- stan_glm(course_evaluation ~ ., data = select(teaching_train, - prof_number, - prof_ave_rating, - age_binned), refresh = 0) lm_min ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ ave_teaching_qual ## observations: 371 ## predictors: 2 ## ------ ## Median MAD_SD ## (Intercept) -0.1 0.2 ## ave_teaching_qual 1.0 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.4 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg lm_max ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ . ## observations: 371 ## predictors: 14 ## ------ ## Median MAD_SD ## (Intercept) 0.0 0.3 ## age 0.0 0.0 ## prof_ave_beauty_rating 0.0 0.0 ## ave_teaching_qual 0.9 0.1 ## tenured 0.0 0.1 ## minority 0.0 0.1 ## female -0.1 0.0 ## formal 0.1 0.1 ## non_native_english -0.2 0.1 ## tenure_track 0.0 0.1 ## eval_response_rate 0.0 0.0 ## students 0.0 0.0 ## lower 0.0 0.0 ## multiple_prof -0.1 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.4 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg Comparing the log scores from the leave-one-out cross validation (elpd_loo) the minimal model performs slightly better. So, Ill use the minimal model as the starting point for building a better model. stan_glm_metric_summary(lm_min, as_kable = TRUE) Metric Estimate SE elpd_loo -173.0884824 16.4984436 p_loo 3.1546102 0.5467246 looic 346.1769648 32.9968872 r2 0.5398558 0.0377842 stan_glm_metric_summary(lm_max, as_kable = TRUE) Metric Estimate SE elpd_loo -174.0492755 17.1412991 p_loo 14.9007741 1.4595946 looic 348.0985510 34.2825982 r2 0.5376852 0.0390503 stan_glm_loo_comp(lm_min, lm_max, as_kable = TRUE) Metric lm_min lm_max elpd_diff 0.0000000 -0.9607931 se_diff 0.0000000 4.6783376 elpd_loo -173.0884824 -174.0492755 se_elpd_loo 16.4984436 17.1412991 p_loo 3.1546102 14.9007741 se_p_loo 0.5467246 1.4595946 looic 346.1769648 348.0985510 se_looic 32.9968872 34.2825982 r2 0.5398558 0.5376852 4.3.3 Iteration 1: Adding two continuous variables Next I try adding two continuous variables (prof_ave_beauty_rating and eval_response_rate) as predictors. These were the two variables after ave_teaching_qual with the next highest (albeit low) levels of correlation with course_evaluation. After adding these variables the model performance is the same as the minimal model. So, Ill keep working with the minimal model as the best performing model for now. lm_1 &lt;- stan_glm(course_evaluation ~ ave_teaching_qual + eval_response_rate + prof_ave_beauty_rating, data = teaching_train, refresh = 0) stan_glm_loo_comp(lm_min, lm_1, as_kable = TRUE) Metric lm_min lm_1 elpd_diff 0.0000000 -0.2398614 se_diff 0.0000000 2.4155078 elpd_loo -172.8486210 -173.0884824 se_elpd_loo 16.9855842 16.4984436 p_loo 5.2861458 3.1546102 se_p_loo 0.7842139 0.5467246 looic 345.6972420 346.1769648 se_looic 33.9711684 32.9968872 r2 0.5423901 0.5398558 4.3.4 Iteration 2: adding in discrete variables Next I try adding in the three discrete variables which had non-zero coefficients in the maximal model (non_native_english + female + multiple_prof). Again this doesnt result in any improvements to model performance as measured by the cross validation log score (elpd_loo) lm_2 &lt;- stan_glm(course_evaluation ~ ave_teaching_qual + # add discrete predictors non_native_english + female + multiple_prof, data = teaching_train, refresh = 0) stan_glm_loo_comp(lm_min,lm_2, as_kable = TRUE) Metric lm_min lm_2 elpd_diff 0.0000000 -0.9209362 se_diff 0.0000000 2.2909954 elpd_loo -173.0884824 -174.0094186 se_elpd_loo 16.4984436 16.7649547 p_loo 3.1546102 6.3682923 se_p_loo 0.5467246 0.8387327 looic 346.1769648 348.0188372 se_looic 32.9968872 33.5299094 r2 0.5398558 0.5386075 lm_2 ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ ave_teaching_qual + non_native_english + ## female + multiple_prof ## observations: 371 ## predictors: 5 ## ------ ## Median MAD_SD ## (Intercept) 0.1 0.2 ## ave_teaching_qual 1.0 0.0 ## non_native_english -0.1 0.1 ## female -0.1 0.0 ## multiple_prof 0.0 0.0 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.4 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg 4.3.5 Iteration 3: adding in interactions Next I try adding in interaction between the discrete predictors and ave_teaching_qual. As I would assume that if they are associated with the course_evaluation scores given by students, they will also be associated with the prof_ratings given by students (which have been average to give ave_teaching_qual). Adding in these interactions seems to make things worse, and the minimal model remains the best model to date (in terms of log score). lm_3 &lt;- stan_glm(course_evaluation ~ ave_teaching_qual + non_native_english + female + multiple_prof + # add interactions between discrete predictors and the continuous predictor ave_teaching_qual:non_native_english + ave_teaching_qual:female + ave_teaching_qual:multiple_prof, data = teaching_train, refresh = 0) stan_glm_loo_comp(lm_min,lm_3, as_kable = TRUE) Metric lm_min lm_3 elpd_diff 0.0000000 -3.1455579 se_diff 0.0000000 2.4034963 elpd_loo -173.0884824 -176.2340403 se_elpd_loo 16.4984436 16.6240223 p_loo 3.1546102 8.6670133 se_p_loo 0.5467246 1.0750751 looic 346.1769648 352.4680806 se_looic 32.9968872 33.2480447 r2 0.5398558 0.5342003 lm_3 ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ ave_teaching_qual + non_native_english + ## female + multiple_prof + ave_teaching_qual:non_native_english + ## ave_teaching_qual:female + ave_teaching_qual:multiple_prof ## observations: 371 ## predictors: 8 ## ------ ## Median MAD_SD ## (Intercept) -0.1 0.3 ## ave_teaching_qual 1.0 0.1 ## non_native_english 0.4 1.4 ## female 0.3 0.4 ## multiple_prof 0.1 0.4 ## ave_teaching_qual:non_native_english -0.1 0.4 ## ave_teaching_qual:female -0.1 0.1 ## ave_teaching_qual:multiple_prof 0.0 0.1 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.4 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg 4.3.6 Iteration 4: reducing number of discrete predictors used to simplify the model Next I tried simplifying the model by reducing the number of discrete predictors and interaction. I leave non_native_english speaker in as a predictor as in the previous model it had a non-zero coefficient (albeit with considerable uncertainty around it). However, the revised model also performs marginally worse than the minimal model. lm_4 &lt;- stan_glm(course_evaluation ~ ave_teaching_qual + non_native_english + ave_teaching_qual:non_native_english, data = teaching_train, refresh = 0) stan_glm_loo_comp(lm_min,lm_4, as_kable = TRUE) Metric lm_min lm_4 elpd_diff 0.0000000 -1.1228167 se_diff 0.0000000 1.8001823 elpd_loo -173.0884824 -174.2112991 se_elpd_loo 16.4984436 16.5730517 p_loo 3.1546102 5.3133395 se_p_loo 0.5467246 0.8779247 looic 346.1769648 348.4225982 se_looic 32.9968872 33.1461033 r2 0.5398558 0.5383073 lm_4 ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ ave_teaching_qual + non_native_english + ## ave_teaching_qual:non_native_english ## observations: 371 ## predictors: 4 ## ------ ## Median MAD_SD ## (Intercept) 0.0 0.2 ## ave_teaching_qual 1.0 0.0 ## non_native_english 0.5 1.4 ## ave_teaching_qual:non_native_english -0.2 0.4 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.4 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg lm_5 &lt;- stan_glm(course_evaluation ~ eval_response_rate + prof_ave_beauty_rating + non_native_english + female + multiple_prof + ave_teaching_qual:non_native_english + ave_teaching_qual:female + ave_teaching_qual:multiple_prof, data = teaching_train, refresh = 0) stan_glm_loo_comp(lm_min,lm_5, as_kable = TRUE) Metric lm_min lm_5 elpd_diff 0.0000000 -77.887273 se_diff 0.0000000 13.981130 elpd_loo -173.0884824 -250.975755 se_elpd_loo 16.4984436 16.917053 p_loo 3.1546102 9.256360 se_p_loo 0.5467246 1.197869 looic 346.1769648 501.951510 se_looic 32.9968872 33.834106 r2 0.5398558 0.301848 lm_5 ## stan_glm ## family: gaussian [identity] ## formula: course_evaluation ~ eval_response_rate + prof_ave_beauty_rating + ## non_native_english + female + multiple_prof + ave_teaching_qual:non_native_english + ## ave_teaching_qual:female + ave_teaching_qual:multiple_prof ## observations: 371 ## predictors: 9 ## ------ ## Median MAD_SD ## (Intercept) 3.7 0.1 ## eval_response_rate 0.0 0.0 ## prof_ave_beauty_rating 0.1 0.0 ## non_native_english -0.5 1.6 ## female -2.8 0.4 ## multiple_prof -2.1 0.5 ## non_native_english:ave_teaching_qual 0.1 0.4 ## female:ave_teaching_qual 0.6 0.1 ## multiple_prof:ave_teaching_qual 0.5 0.1 ## ## Auxiliary parameter(s): ## Median MAD_SD ## sigma 0.5 0.0 ## ## ------ ## * For help interpreting the printed output see ?print.stanreg ## * For info on the priors used see ?prior_summary.stanreg 4.3.7 Summary of model fitting process This looks like about the end of the road in terms of trying out combinations of features to improve the predictive performance of the model (as estimated by the log score). I could try some feature engineering on the predictors and outcome variable (e.g. centering and scaling, transformation aimed at producing more normal distributions). However, in light of the findings of the 4 model fitting iterations above I think it is unlikely that feature engineering would considerably improve the performance of the model. Unfortunately, with the data to hand it looks like it is going to be challenging greatly improve the models performance. So, ideally it would be a case of conducting further studies gather data on a wider range of variables. Perhaps including variables that qualitatively encode something of a professor teaching style/approach. And/or variables such as the average marks awarded to students on the course before the evaluation had taken place. The r squared value for the minimal and best performing model is 0.542. Which indicates that the very simple one-predictor model is explaining just over half the variance in course_evaluation. "],["visualising-green-space-deprivation-across-the-english-regions.html", "Chapter 5 Visualising green space deprivation across the English regions", " Chapter 5 Visualising green space deprivation across the English regions In another notebook I conducted an exploratory analysis of green space deprivation data published by Friends of the Earth in the Autumn of 2020. Friends of Earth rated the extent of green space deprivation in each neighborhood (each middle-layer-super-output-area in language of UK administrative geography) in England. One of the interesting findings from this exploratory data analysis was that London has a much higher rate of neighborhoods experiencing the worst forms of green space deprivation than other English regions. I wanted to explore ways to visually communicate this finding, looking beyond a geographic representation where it would have challenging to convey this finding due to differences in the density of regions across the UK. In the end I settled up the waffle chat below (basically a slight adaptation on the familiar bar chart). The design of the visualization was inspired by this #TidyTuesday post. Reflecting on the process of producing the plot, I noted: The difference font styles and colour palettes can make to the visual identity of a plot; That perhaps the additional effort and time required to produce a waffle plot (verses a standard bar chart) would be difficult to justify in the future; That I had learnt a considerable amount using the theme function from ggplot2 to customize the visual design of a plot. I hope these learnings will be useful in producing visualisations which convey the findings of the data science projects I am working on at the moment. I also thought it was important to avoid getting caught up in the visual design of plots being produced as part of statistical analyses. Given the small, technical audiences for the later, the default themes from ggplot2 are likely to be more than good enough. In the code below, I read in, clean and reshape the data before producing the plot itself. # import core libraries library(tidyverse) library(janitor) # for cleaning the variables names in the imported data library(readxl) # for reading in data in an Excel format # import libraries used for building the plot library(waffle) library(hrbrthemes) library(patchwork) library(ggtext) library(viridisLite) library(ggtext) # import fonts to be used in the final figure import_roboto_condensed() #****************************************************************************** # READ IN AND CLEAN DATA #****************************************************************************** #&#39; reads in the Friends of the Earth Green Space Dataset #&#39; #&#39; @return: a tibble with consistently named variables read_foe_data &lt;- function(){ green_space &lt;- read_excel(&quot;./data/(FOE) Green Space Consolidated Data - England - Version 2.1.xlsx&quot;, sheet = &quot;MSOAs V2.1&quot;) %&gt;% # inconsistent naming conventions for variables are used in the source data # some needed to clean names for consistency (variables names are now ) clean_names() return(green_space) } # read in FOE data foe_green_space &lt;- read_foe_data() # focus on variables of interest for visualisation foe_green_space_focus &lt;- foe_green_space %&gt;% select(msoa_code, msoa_name, la_code, green_space_deprivation_rating) # The FOE green space data doesn&#39;t include a variable for the region each # MSOA/local authority lies within So, import a look up table from the ONS, # which maps between Local Authority Districts and Regions LAD_to_region &lt;- read_csv(&quot;./data/Local_Authority_District_to_Region__December_2019__Lookup_in_England.csv&quot;) %&gt;% select(-FID, -LAD19NM) # merge the FOE green space and ONS region tibbles green_space_regions &lt;- foe_green_space_focus %&gt;% left_join(LAD_to_region, by = c(&quot;la_code&quot; = &quot;LAD19CD&quot;)) %&gt;% rename(region = RGN19NM, region_code = RGN19CD) #****************************************************************************** # RESHAPE DATA FOR PLOTTING #****************************************************************************** #&#39; adds a column of shorten region names which are easier read/use in plots #&#39; #&#39; @param df: a data frame including a region variable (with standard ONS region names) #&#39; #&#39; @return: an amended data frame with an additional column of short region names add_region_short_names &lt;- function(df){ res &lt;- df %&gt;% mutate(region_short = case_when( region == &quot;East Midlands&quot; ~ &quot;E. Mids.&quot;, region == &quot;East of England&quot; ~ &quot;E. Eng.&quot;, region == &quot;North East&quot; ~ &quot;NE&quot;, region == &quot;North West&quot; ~ &quot;NW&quot;, region == &quot;South East&quot; ~ &quot;SE&quot;, region == &quot;South West&quot; ~ &quot;SW&quot;, region == &quot;West Midlands&quot; ~ &quot;W. Mids&quot;, region == &quot;Yorkshire and The Humber&quot; ~ &quot;Yorks.&quot;, region == &quot;London&quot; ~ &quot;Lon.&quot;, TRUE ~ region), region_short = str_to_upper(region_short) ) return(res) } # process the FOE data for plotting including: # counting the number of neighborhoods with each rating in each region # and, adding in shorter easier to plot region names region_ratings_counts &lt;- green_space_regions %&gt;% filter(!is.na(region)) %&gt;% group_by(region, green_space_deprivation_rating) %&gt;% count() %&gt;% add_region_short_names() # define an indicator variable identifying if urgent action is needed # in a neighborhood to address green space deprivation # (i.e. identify if each neighborhood is rated D or E) # this indicator variables is used for ordering the regions in the plot urgent_action_counts &lt;- green_space_regions %&gt;% add_region_short_names() %&gt;% mutate(D_or_E = green_space_deprivation_rating %in% c(&quot;D&quot;, &quot;E&quot;)) %&gt;% group_by(region_short) %&gt;% summarise(num_D_or_E = sum(D_or_E)) %&gt;% na.omit() %&gt;% arrange(desc(num_D_or_E)) # Take the counts of the numbers of neighborhoods in each region with each # green space deprivation rating, and add in the urgent action counts for each # region region_ratings_counts &lt;- region_ratings_counts %&gt;% left_join(urgent_action_counts) %&gt;% mutate(region_short = factor(region_short, levels = pull(urgent_action_counts, region_short)), green_space_deprivation_rating = factor(green_space_deprivation_rating, levels = c(&quot;E&quot;, &quot;D&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;))) %&gt;% # needed to reorder ratings in each column arrange(green_space_deprivation_rating) #****************************************************************************** # PRODUCE THE PLOT #****************************************************************************** # define the colour palette to be used in the plot pal &lt;- viridis(n = 5, alpha = 1, begin = 0, end = 1, direction = 1,option =&quot;A&quot;) bg_colour &lt;- &quot;#dee3e3&quot; # define the degree transparency used to de-emphasis neighborhoods where urgent # action to address green space is not required alpha_hr &lt;- 0.25 # create the base of the plot waf &lt;- region_ratings_counts %&gt;% ggplot(aes(fill = green_space_deprivation_rating, values = n / 10)) + expand_limits(x=c(0,0), y=c(0,0)) + coord_equal() + theme_ipsum_rc(grid=&quot;&quot;) + theme_enhance_waffle() + labs(fill = NULL, colour = NULL) # add various layers to the base of the blot to produce the final figure waf + geom_waffle(n_rows = 5, size = 0.5, flip = TRUE, colour = bg_colour) + # de-emphasis neighborhoods where urgent action to address green space # is not required scale_fill_manual(values = c(pal[1], pal[2], alpha(pal[3], alpha_hr), alpha(pal[4], alpha_hr), alpha(pal[5], alpha_hr))) + # immitate a bar chart through facetting facet_wrap(~region_short, nrow = 1, strip.position = &quot;bottom&quot;) + # format legend labs(fill = &quot;Green space\\ndeprivation\\nrating&quot;) + guides(fill = guide_legend(reverse = TRUE)) + # theme settings for plot area from example code theme(panel.spacing.x = unit(0, &quot;npc&quot;), strip.text.x = element_text(hjust = 0.5)) + # custom theme settings for plot area theme(plot.background = element_rect(fill = bg_colour, colour = NA), text = element_text(family = &quot;Roboto Condensed&quot;), legend.title = element_text(family = &quot;Roboto Condensed&quot;), plot.margin = margin(b = 1, r = 25, l = 25), ) + # add text areas to plot plot_annotation( title = str_to_upper(&quot;Green space deprived neighborhoods across the English regions&quot;), subtitle = &quot;\\nFriends of the Earth analysed green space deprivation across England. They gave the most green space deprived neighborhoods (i.e. msoas) a rating of E, and the least a rating of A. Urgent action is needed to address green space deprivation in neighborhoods rated D and E. The number of neighborhoods where urgent action is needed is much higher in London than in other regions of England. &quot;, caption = &quot;Note: each square represents 10 neighborhoods | Source: Friends of the Earth - England&#39;s Green Space Gap&quot;, # custom theme settings for text areas theme = theme(text = element_text(family = &quot;Roboto Condensed&quot;), plot.background = element_rect(fill = bg_colour, colour = NA)) ) # option to save the final figure as a .png visualisation_output = FALSE if(visualisation_output) ggsave(&quot;num_neigh_gs_dep.png&quot;, width = 10, height = 5, units = &quot;in&quot;) "]]
