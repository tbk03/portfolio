# Predicting course evaluation scores

```{r, include = FALSE}
# global settings for producing html output
knitr::opts_chunk$set(warning = FALSE,  # remove for readability
                      message = FALSE)#,  # remove for readability
                      #cache = TRUE)   # for inclusion during coding
```

```{r}
# import packages used in this notebook
suppressPackageStartupMessages({
  suppressWarnings({
    
    # core libraries
    library(tidyverse)
          
    # helper libraries
    library(janitor)  # for cleaning names
    library(skimr)    # for summary statistic reports
    library(glue)     # for string formatting
    library(infer)    # for exploratory inference
  })
})



```

```{r}
# global setting for notebook
theme_set(theme_light())

# import my own plotting functions for styling plots
source("plotting_functions.R")

# import my own functions for producing summary stats (which use on skimr)
source("stats_functions.R")
```

## Data importing and cleaning

### Importing the data

The data I am using in this project is available in the github repository for the book 'Regression and Other Stories'. So, I can read in the csv directly from the repository.

```{r}
# set url for dataset
data_url <- "https://raw.githubusercontent.com/avehtari/ROS-Examples/master/Beauty/data/ProfEvaltnsBeautyPublic.csv"

# read data from github
teaching_eval <- read_csv(data_url) %>% 
  janitor::clean_names() # try to clean up names

# display dataframe for initial inspection
glimpse(teaching_eval)
```

Next I focused down on variables within the dataset to be used in the exploratory data analysis and modeling below. I reviewed the [data code book](https://github.com/avehtari/ROS-Examples/blob/master/Beauty/data/ProfEvaltnsBeautyPublic.log) to identify the outcome variable and promising potential predictor variables. A number of variables have been dropped in the process of focusing down the dataset , as I thought these dropped variable were unlikely to important predictors of `course_evaluation` scores. This was a judgment based on my experiences of working in various universities over a decade or so.

Due to the nature of the variable names in the dataset, automatically cleaning all variables names to make readable and easy to interpret was not possible. So, in the process of focusing down the dataset I renamed various variables to make them easier to remember, interpret and work with. I also reorder the variables to group them in a more logical order: an outcome variable, followed by potential predictors relating to the professor, and then potential predictors relating to the class.

```{r}
# focus down on features in the dataset with a good potential to act as predictors
teaching_eval_focus <- teaching_eval %>%
  
  select(
    
    # select and rename outcome variable
    course_evaluation = courseevaluation,
    
    # select and rename potential predictor variables relating to the professor
    tenured:age,
    prof_ave_rating = profevaluation,
    prof_ave_beauty_rating = btystdave,
    female, formal, 
    non_native_english = nonenglish,
    tenure_track = tenuretrack,
    
    # select and rename potential predictor relating relating to the course 
    lower,
    multiple_prof = multipleclass,
    one_credit = onecredit,
    eval_response_rate = percentevaluating,
    students,
    class1:class30 # dummy variables
    
  ) %>% 

  # Move a potential indentifer variable to the front of the dataframe 
  # and rename for consistency
  rename(prof_number = profnumber)  %>% 
  relocate(prof_number, .before = course_evaluation)

teaching_eval_focus  
```

### Checking for missing values

Next I checked for missing data. While importing the data I notice there were 30 dummy variables used to identify the class being evaluated. I wanted to look in a bit more detail at potential missing values across the dummy variables. So, first I removed the dummy variables and looked at all the other variables. From the missing values check below it looks like this dataset has already to processed as not `NA`s have been identified.

```{r}
visdat::vis_miss(teaching_eval_focus %>% 
                   select(-starts_with("class")))
```

At first glance the there are no `NA`s identified for the dummy variables, for all pairs of observations of observations and dummy variables either a `1` or a `0` is recorded.

```{r}
visdat::vis_miss(teaching_eval_focus %>% 
                   select(starts_with("class")))

```

However, if looking at these dummy variables in more detail it is clear that a lot of data is missing. By summing the all the dummy variables for a given observation, we can see if any classes are recorded for each observation. I would expect this to be `1` where a class is recorded and `0` where a class is not recorded. Hence, in this context a `0` actually corresponds to a missing value and in the plot below we can see that for approximately two thirds of observations a class is not recorded.

```{r}
# check if each evaluation has a class (1-30) associated with it
# by processing the dummy variables
dummy_var_row_sum <- teaching_eval_focus %>%
  select(class1:class30) %>% 
  rowSums() %>% 
  as_tibble() %>% 
  rename(class_indentified = value) # not all evaluations have a class                                              # associated with them

visdat::vis_miss(dummy_var_row_sum %>% 
                   na_if(0))
```

Working with the dummy variables in the analysis and modeling below would entail dropping two thirds observations and `n` = `r nrow(teaching_eval_focus)`. So, although I recognized the course identifier dummy variables as potentially important predictors of `course_evaluation` I dropped them from the analysis, rather than loose so many observations from the dataset.

```{r}
teaching_eval_clean <- teaching_eval_focus %>% 
  select(-starts_with("class"))

glimpse(teaching_eval_clean)
```

## Exploratory data analysis

### The response variable

So, I started the exploratory data analysis quickly looking at summary statistics for `course_evaluation`, which is the response variable in this exercise. This first things I noted was that the both the mean and median `course_evaluation` score is 4, and the mini-histogram indicates a potential left skew of the data. So, it appears that the `course_evaluation` scores tend to be towards the higher end of the 5 point scale. I want to look at this in a little more detail, so moved on to plotting a full histogram.

```{r, skimr_include_summary = FALSE}
skim_minimal(teaching_eval_clean, course_evaluation)
```

```{r}

num_evals <- nrow(teaching_eval_clean)
num_evals_three_plus <- nrow(
  teaching_eval_clean[teaching_eval_clean$course_evaluation >= 3,]
)

perc_evals_three_plus <- round((num_evals_three_plus / num_evals) * 100, 0)

anno_text <- glue::glue("{perc_evals_three_plus}% of courses have
                        an average evaluation
                        score of 3 or above")


# create plot base
p <- ggplot(data = teaching_eval_clean,
            mapping = aes(x = course_evaluation)) +
  
  # add a reference line
  geom_vline(xintercept = 3, colour = "grey70", size = 1.5, linetype = "longdash") +
  geom_rect(xmin = 3, xmax = 5.05, ymin = 0 , ymax = 40, fill = "grey92", alpha = 0.05) +
  
  # add annotation
  geom_label(x = 3.2, y = 35,
             label = anno_text, size = 3, colour = "grey20") +
  
  # create histogram with minimal styling
  geom_minimal_hist(ylim=c(0,40),
                    binwidth = 0.1) +
  
  # adjust x axis labels for readability
  scale_x_continuous(breaks = seq(0,5,0.5)) +
  expand_limits(x = 0) +
  
  # adjust y axis labels for readability
  scale_y_continuous(breaks = seq(0,40,10)) +
  
  # add plot labels
  labs(title = "Course evaluation scores tend to be towards\nthe top end of the five point scale\n",
       x = "\nAverage course evaluation score\n(given by students)",
       y = "Number of courses\n") 
```

So, it full plot there is some indication of left skew, in the form of the longer tail to the left of the plot. Also, evident is the fact there is a hard limit (of 5) on the maximum `course_evaluation` score, which accentuates the impression of the skew.

```{r}
p
```

So, I took a quick look at `course_evaluation` centered around the mean. This looks like a close enough approximation to the normal distribution to me. So, ahead of modeling I don't need to think about transforming `course_evaluation` scores to address the limited amount of skew evident in the plot. However, it might be worth centering the `course_evaluation` scores as this might help make the regression coefficients easier to interpret. But I'll come back to that later on.

```{r}
course_evaluation_mean = mean(teaching_eval_clean$course_evaluation)

p <- ggplot(data = teaching_eval_clean,
            mapping = aes(x = course_evaluation - course_evaluation_mean)) +
  geom_minimal_hist(ylim=c(0,40),
                    binwidth = 0.1)

p +
  labs(title = "Centering course evaluation scores makes it\neasier to see the extent of skew\n",
       x = "\nCentred average course evaluation score",
       y = "Number of courses\n")

```

### Potential explanatory variables

So, I moved on to looking at the potential explanatory variables within the dataset. Remembering that when I imported the data I had thought about the potential explanatory variables as belonging to two distinct groups: (1) Professor related variables; and, (2) course related variables. So, below I take at look at these two groups of variables in turn.

#### Professor related explanatory variables

The first thing I noticed when looking at the summary statistics for the Professor related variables was that all the variables are numeric. But that there is categorical data present which has already coded as binary variables. So, I decided to split the variables again into groups of binary variables and continuous variables, before looking at the data in more detail.

```{r, skimr_include_summary = FALSE}

# focus on variables relating to professors
explanatory_variables_prof <- teaching_eval_clean %>% 
  select(prof_number, tenured:tenure_track)

# produce summary statistics
skim_minimal(explanatory_variables_prof %>% 
              select(-prof_number), # no need to include identifer variable
             show_data_completeness = FALSE)

```

To do this I wrote a quick function to identify binary variables within the dataset.

```{r}
#' Checks if a column is a binary variable 
#' i.e. checks if a column only contains zeros and ones
#'
#' @param col: A column of dataframe passed by a TidyVerse function
#'
#' @return: A boolean

is_binary_var <- function(col){
  
  # identify unique entries in column
  u <- col %>% 
    unique()
  
  # check if only 0s and 1s a are within the column and return as appropriate
  if (identical(sort(u), c(0,1))) return(TRUE)
  else return(FALSE)
}


```

##### Continuous variables

There are only three continuous variables relating to Professors:

-   the average rating given by students to each Professor teaching the course (`prof_ave_rating`);

-   the average beauty rating by students to each Professor teaching the course (`prof_ave_beauty_rating)`;

-   and, `age`.

Immediately below are the summary statistics for each of the three variables. I then take a more detailed look at each one of these variables in turn.

```{r, skimr_include_summary = FALSE}
# identify continous variables within the dataset
explanatory_variables_prof_cont <- explanatory_variables_prof %>% 
  select(where(~!is_binary_var(.))) 

# show summary statistics
skim_minimal(explanatory_variables_prof_cont %>% 
              select(-prof_number),
             show_data_completeness = FALSE)
```

At first glance the distribution of `prof_ave_rating` is very similar to the distribution of the response variable (`course_evaluation)` .

```{r}
plot_title <- "(Similarly to course evaluation scores)\nProfessor rating tends to be towards the upper end of the five point scale\n"

ggplot(data = explanatory_variables_prof_cont,
       mapping = aes(x = prof_ave_rating)) +
  geom_minimal_hist(ylim = c(0,45), binwidth = 0.1) +
  expand_limits(x = 0)+
  
  labs(title = plot_title,
       x = "Professor rating",
       y = "Number of ratings\n")
```

The plot below compares `course_evaluation` and `prof_ave_rating` at each observation in the dataset. This shows that for a given course-professor pair the `prof_ave_rating` tends to be higher than the `course_evaluation` score.

```{r, fig.height= 7.5}
teaching_eval_clean %>% 
  
  # focus on the two variables to be comparted for each observation
  select(course_evaluation, prof_ave_rating) %>% 
  arrange(prof_ave_rating) %>% 
  
  # put the data in a long format for plotting
  mutate(id = 1:nrow(.), .before = course_evaluation) %>% 
  pivot_longer(cols = c(course_evaluation, prof_ave_rating),
               names_to = "score_type", values_to = "score") %>% 
  
  # create plot base
  ggplot(mapping = aes(x = score, y = id)) +
  
  # add points for prof rating and course evaluation
  # and connect each pair of points with a line
  geom_line(aes(group = id), colour = "grey50") +
  geom_point(aes(colour = score_type)) + 
  
  # show full x axis
  expand_limits(x = 0) +
  # remove gap between x axis and plot
  coord_cartesian(ylim = c(0, nrow(teaching_eval_clean)+5),
                  xlim = c(0, 5.1),
                  expand = FALSE) +
  
  # add plot labels
  labs(title = "But Professor ratings tend to be higher than course evaluation scores",
       subtitle = "Both are on the same five point scale\n") + 
  
  # format legend to remove clutter
  scale_color_brewer(palette = "Set1",
                     labels = c("Course evaluation score",
                                  "Professor rating")) +
  guides(colour = guide_legend(title = NULL)) +
  
  # adjust theme to remove clutter
  remove_all_grid_lines() +
  theme(axis.title.y  = element_blank(),
        axis.text.y = element_blank(),
        axis.line.y = element_blank(),
        legend.position = "top",
        legend.text = element_text(size = 10))
```

Professors can appear multiple times in the dataset, where they have been involved in teaching multiple courses. The `prof_ave_rating` is the average of the ratings given by students to the professor for an individual course. The plot below shows the `prof_ave_rating`s for 10 randomly selected professors. So, highlighting that as expected there is some variability in the average ratings that professors received from course to course.

```{r}
# ensure the sample random sample of professors is take when notebook is rerun
set.seed(100)

explanatory_variables_prof_cont %>% 
  
  # select a random sample of professors
  group_by(prof_number) %>%
  nest() %>% 
  ungroup() %>% 
  slice_sample(n = 10) %>% 
  mutate(prof_number = factor(prof_number)) %>% # convert to factor 
                                                # to simplify plotting
  unnest() %>% 
  
  # produce plot
  ggplot(aes(prof_ave_rating, prof_number)) +
  
  # jitter slightly to make overlapping points easier to see
  geom_jitter(alpha = 0.5, height = 0.1) +

  # adjust axis for readibility
  expand_limits(x = 0)+
  coord_flip() +
  
  # add plot labels
  labs(y = "Professor identification number",
       x = "Course evaluation score",
       title = "There is some variablility in the evaluation scores professors\n received from course to course",
       subtitle = "Showing course evaulation scores for 10 randomly selected professors")
```

So, averaging ratings for a given professor over multiple courses might give an idea of the quality of teaching delivered in general by a given professor. I'll call this mean of average ratings over multiple course `ave_teaching_qual` just to avoid confusing terminology created with the multiple levels of averaging taking place. I'll return to this estimate of teaching quality delivered by each professor later on, as it may be a potentially useful predictor of a `course_evaluation` score.

```{r}
prof_ave_teach_qual <- explanatory_variables_prof_cont %>% 
  group_by(prof_number) %>%
  summarise(ave_teaching_qual = mean(prof_ave_rating))

explanatory_variables_prof_cont <- explanatory_variables_prof_cont %>% 
  left_join(prof_ave_teach_qual)
```

Moving on to the next professor related continuous variable the `prof_ave_beauty_rating`. There is a single beauty rating for each professor. I confirmed this in the code below by confirming the standard deviation of each Professor's `prof_ave_beauty_rating` is zero.

```{r}
# check if the beauty of professors has been rated seperately for each course
explanatory_variables_prof_cont %>% 
  
  group_by(prof_number) %>% 
  summarise(beauty_rating_sd = sd(prof_ave_beauty_rating)) %>% 
  
  # I am not sure why NAs are being produced here
  # I checked the dataframe manually and they look like they relate to empty rows 
  # at the end of the dataframe. After a quick Google I could find an obvious
  # reason for these empty rows, so I decided to leave this issues for now.
  distinct(beauty_rating_sd)

```

Looking at the `prof_ave_beauty_rating`s in the plot below, it looks like again the rating were on five point scale and have already been centered. I am going to assume zero corresponds to average beauty on a Likhert scale (very unattractive, unattractive, average attractiveness, attractive, very attractive). Then it looks like there may be some tendency for students to rate their Professors as being unattractive or around average. Remembering that the ratings plotted below are averages and assuming that students were not asked to rate attractiveness on a more granular scale, where ratings of for example -0.1 could be given. A Likhert scale seems much more likely.

```{r}
ggplot(explanatory_variables_prof_cont,
       aes(prof_ave_beauty_rating)) +
  geom_minimal_hist(ylim = c(0,70), binwidth = 0.2) +
  
  labs(title = "Do students tend to rate Professor as average\nor below average attactiveness?",
       x = "Average beauty rating",
       y = "Number of professors\n")
```

Finally, for the Professor related continuous variables I plot the distribution of `age`. There is too much too note here. Other than the shape of the histogram is rather sensitive to the width of the bins.

```{r}
ggplot(explanatory_variables_prof_cont,
       aes(age)) +
  geom_minimal_hist(ylim = c(0,110), binwidth = 5) +
  
  labs(title = "The distribution of the ages of Professors begins to approximate\n the normal distriubtion with an appropriate binwidth")
```

This made wonder if it might be helpful during the modelling to have a binned (i.e. discretised) age variable which groups professors by age.

```{r}
# create discrete age variable (5 year age groups starting at age 20)
explanatory_variables_prof_cont <- explanatory_variables_prof_cont %>% 
  mutate(age_binned = cut_width(age, width = 5, boundary = 20))
```

##### Discrete variables

Moving on to look at the discrete (primarily binary) Professor related variables. I start by looking at the summary statistics. There is not too much to do in terms of visualisation of these variables at this point.

For the binary variables the mean value (shown in the summary stats below) indicates the proportion of professors with the characteristic associated with the variable (e.g. The proportion of professors who are `tenured` is 0.546). So, we can see that binary variables differ in the extent to which they partition the professors in to groups of equal or unequal size.

```{r, skimr_include_summary = FALSE}
# identify binary variables within the dataset
explanatory_variables_prof_discrete <- explanatory_variables_prof %>% 
  select(where(~is_binary_var(.))) %>% 
  
  # add back in identifer variable and reorder for readability
  bind_cols(select(explanatory_variables_prof, prof_number)) %>% 
  relocate(prof_number, .before = tenured) %>% 
  
  # add in age group variable group
  bind_cols(select(explanatory_variables_prof_cont, age_binned))

# show summary statistics
skim_minimal(explanatory_variables_prof_discrete %>% 
              select(-prof_number), 
             show_data_completeness = FALSE)
```

Looking at the binary variables we might expect to some interactions between. For example, I wonder if a larger proportion of professors will be `tenured` if they are not `female`, and below I do a quick bit of exploratory inference to see if I am correct.

So, H~0~ is that there is no difference in the proportions of female and male professor who are `tenured`. And, H~A~ is that there is a difference between the proportions of female and male professor who are `tenured`. Below I use the `infer` package to test H~0~ (assuming a significance level of 0.05)

-   First, calculating the observed difference in proportions;

-   Then, generating the null distribution (i.e. the distribution of differences we could expect from a large number of samples collected using the same methods as the observed sample) using bootstrap resampling.

-   Finally, comparing the observed difference in proportion with the null distribution. We can see on the plot that H~0~ can be rejected and the p-value of observed difference (approx. equal to 0) confirms this. So, in the population of Professors from which the current observations were sampled from we can be 95% confident that there is a difference between the proportions of female and male professor who are `tenured`. And, given the observed difference in proportions 0.27 (male - female) we would expect more male than female professors to be `tenured`.

```{r}
# calculating the observed difference in proportion of men and women
# with tenure
diff_tenured <- explanatory_variables_prof_discrete %>%
  
  # the test requires catergorical data to be coded as a factor
  mutate(tenured = factor(tenured),
         female = factor(female)) %>% 
  
  specify(tenured ~ female, success = "1") %>% 
  calculate(stat = "diff in props")

# A postive difference indicates higher proportion of men tenured
diff_tenured

# generate the null distribution
null_distn <- explanatory_variables_prof_discrete %>%
  
  # the test requires catergorical data to be coded as a factor
  mutate(tenured = factor(tenured),
         female = factor(female)) %>%
  
  specify(tenured ~ female, success = "1") %>% 
  hypothesize(null = "independence") %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "diff in props")

# visualise distribution under null hypothesis
visualize(null_distn) +
  shade_p_value(obs_stat = diff_tenured, direction = "two-sided") +
  theme_light() +
  remove_all_grid_lines()

# confirm p-value
null_distn %>%
  get_p_value(obs_stat = diff_tenured, direction = "two-sided")

```

I think I'll leave that there in terms of exploratory inference, and just assume for that there interactions between the discrete variables and come back to that issue if and when required in the modeling process.

#### Course related explanatory variables
Having focused on the professor related variables above, I now move on to exploring the course related variables. Similarly to the professor related variables, there are a mix of continuous and binary course related variables. So, below I look at each of two groups of variables separately.

```{r, skimr_include_summary = FALSE}
explanatory_variables_course <- teaching_eval_clean %>% 
  select(lower:students)

skim_minimal(explanatory_variables_course, show_data_completeness = FALSE)
```

##### Continuous variables
Below I show the summary statistics below for the two continuous variables: the evaluation response rate (`eval_response_rate`) and the number of students taking the course (`students`). A quick look suggests that both the variables are heavily skewed. So, I'll look at each in a bit more details below.

```{r, skimr_include_summary = FALSE}
# identify continuous variables within the dataset
explanatory_variables_course_cont <- explanatory_variables_course %>% 
  select(where(~!is_binary_var(.)))

explanatory_variables_course_cont %>% 
  skim_minimal(show_data_completeness = FALSE)
```
**Evaluation response rate:** The histogram below shows that the distribution for `eval_response_rate` is left skewed, with the number of courses (i.e. frequency) increasing with increasing `eval_response_rate` up until around 80% response rate. Above 80% there is some evidence of a decline in the number of course with a given response rate. This makes sense as it would be challenging to get all the students of a given course to participate in an evaluation survey. 

At the moment I am not sure if it will be beneficial to transform this variable ahead of modeling. As rather than approximating a normal distribution, it is also straightforward to interpret the distribution as composed of a linear trend between 0 and 80%. And, of course the choice of bin width of the histogram plays a key role in visual appearance of distribution. Again I'll come back this later during the modelling if needed.

```{r}
# create plot base
ggplot(explanatory_variables_course_cont,
       aes(eval_response_rate)) +
  
  # add the plot itself
  geom_minimal_hist(ylim = c(0, 40), binwidth = 2.5) +
  
  # adjust scales to make them easier to interpret
  scale_x_continuous(labels = scales::percent_format(scale = 1),
                     breaks = c(0,20,40,60,80,100)) +
  
  # add more readible axis titles and a plot title
  labs(x = "\nStudents participating in the course evaluation survey",
       y = "Number of courses",
       title = "Higher participation rates in course evaluation surveys\nare more common than lower participation rates\n")
```


**Number of students:**

```{r}
# create plot base
ggplot(explanatory_variables_course_cont,
       aes(students)) +
  
  # add reference line
  geom_vline(xintercept = 100, colour = "grey90", size = 2, alpha = 0.5) +
  
  # add the plot itself
  geom_minimal_hist(ylim = c(0, 140), binwidth = 10) +
  
  # adjust scales to make the plot easier to interpret
  scale_y_continuous(breaks = seq(0,140,20)) +
  
  # add more readible axis titles and a plot title
  labs(x = "\nNumber of students registered on a course",
       y = "\nNumber of courses\n",
       title = "Most courses have 100 students or less\n but there are courses with much larger numbers of students\n")
```

```{r}
# create plot base
ggplot(explanatory_variables_course_cont,
       aes(students)) +
  
  # add reference line
  geom_vline(xintercept = 100, colour = "grey90", size = 2, alpha = 0.8) +
  
  # add the plot itself
  geom_density(size = 2, colour = "grey70") +
  scale_x_log10() +
  
  remove_all_grid_lines() +
  labs(x = "\nNumber of students registered on a course\n(plotted on a log scale)",
       y = "\nEstimated density\n",
       title = "The distribution of number of students registered per course\nremains left skewed after a log transformation \n")
```

##### Discrete variables
```{r, skimr_include_summary = FALSE}

# identify continuous variables within the dataset
explanatory_variables_course_discrete <- explanatory_variables_course %>% 
  select(where(~is_binary_var(.)))

explanatory_variables_course_discrete %>% 
  skim_minimal()

df <- explanatory_variables_course_discrete%>%
    skim_minimal(show_data_completeness = FALSE)
```


### Relationships between variables

#### Relationships between continuous variables

Look for colinearity of predictors

#### Relationships between discrete variables and continuous variables

Looking for interactions

## Modelling

Remember to split into test and training sets
